{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:99: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  base_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:99: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  base_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:99: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  base_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:101: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:101: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:101: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:103: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:103: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:103: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:105: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_po_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:105: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_po_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:105: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_po_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:107: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:107: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:107: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:109: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:109: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:109: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:111: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:111: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:111: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:113: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_gamma_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:113: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_gamma_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:113: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_gamma_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:115: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:115: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:115: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:117: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:117: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:117: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:120: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  base_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:120: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  base_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:120: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  base_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:122: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:122: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:122: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:124: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:124: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:124: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:126: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_delta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:126: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_delta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:126: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_delta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:128: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:128: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:128: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:130: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:130: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:130: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_gamma_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:132: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:132: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:132: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:134: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:134: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:134: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_learn_theta_ext = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:137: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_tv = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:137: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_tv = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:137: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_tv = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:139: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_tv_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:139: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_tv_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:139: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_tv_learn_delta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:141: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_tv_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:141: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_tv_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:141: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_tv_learn_gamma = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:143: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_tv_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:143: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_tv_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:143: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_tv_learn_theta = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:291: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp1_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'],\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:310: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp1_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EW\n",
      "PO\n",
      "Base\n",
      "Nominal\n",
      "DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:343: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp2_validation_table.set_axis(['eta', 'Epochs', 'DR (learn delta)'], axis=1, inplace=True)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:360: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp2_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO\n",
      "DR\n",
      "DR (learn $\\delta$)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:394: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp3_validation_table.set_axis(['eta', 'Epochs', 'Nom. (learn gamma)', 'DR (learn gamma)',\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:414: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp3_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO\n",
      "Nominal\n",
      "DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:452: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp4_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'],\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:470: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp4_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO\n",
      "Base\n",
      "Nominal\n",
      "DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:509: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'Nom. (gamma)', 'Nom. (theta)',\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:570: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:570: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:570: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:572: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:572: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:572: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:574: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  nom_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:574: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  nom_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:574: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  nom_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:576: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:576: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:576: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_linear = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:578: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:578: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:578: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_2layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:580: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:580: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:580: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net_3layer = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:681: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp5_validation_table.set_axis(['eta', 'Epochs', 'Nom. (linear)', 'DR (linear)',\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/2519059060.py:702: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  exp5_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom. (linear)\n",
      "DR (linear)\n",
      "Nom. (2-layer)\n",
      "DR (2-layer)\n",
      "Nom. (3-layer)\n",
      "DR (3-layer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    }
   ],
   "source": [
    "# Distributionally Robust End-to-End Portfolio Construction\n",
    "# Experiment 1 - General\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Make the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Import E2E_DRO functions\n",
    "from e2edro import e2edro as e2e\n",
    "from e2edro import DataLoad as dl\n",
    "from e2edro import BaseModels as bm\n",
    "from e2edro import PlotFunctions as pf\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "cache_path = \"./cache/exp/\"\n",
    "# cache_path = \"C:\\\\Users\\\\Rafay\\Documents\\\\thesis\\\\ActualWork\\\\Backtester\\\\E2E-DRO\\\\cache\\\\exp\\\\\"\n",
    "####################################################################################################\n",
    "# Experiments 1-4 (with hisotrical data): Load data\n",
    "####################################################################################################\n",
    "\n",
    "# Data frequency and start/end dates\n",
    "freq = 'weekly'\n",
    "start = '2000-01-01'\n",
    "end = '2021-09-30'\n",
    "\n",
    "# Train, validation and test split percentage\n",
    "split = [0.6, 0.4]\n",
    "\n",
    "# Number of observations per window \n",
    "n_obs = 104\n",
    "\n",
    "# Number of assets\n",
    "n_y = 20\n",
    "\n",
    "# AlphaVantage API Key. \n",
    "# Note: User API keys can be obtained for free from www.alphavantage.co. Users will need a free \n",
    "# academic or paid license to download adjusted closing pricing data from AlphaVantage.\n",
    "AV_key = 'W5ACAYR6PEX7L28T'\n",
    "\n",
    "# Historical data: Download data (or load cached data)\n",
    "X, Y = dl.AV(start, end, split, freq=freq, n_obs=n_obs, n_y=n_y, use_cache=True,\n",
    "            save_results=False, AV_key=AV_key)\n",
    "\n",
    "# Number of features and assets\n",
    "n_x, n_y = X.data.shape[1], Y.data.shape[1]\n",
    "\n",
    "####################################################################################################\n",
    "# E2E Learning System Run\n",
    "####################################################################################################\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Performance loss function and performance period 'v+1'\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "\n",
    "# Weight assigned to MSE prediction loss function\n",
    "pred_loss_factor = 0.5\n",
    "\n",
    "# Risk function (default set to variance)\n",
    "prisk = 'p_var'\n",
    "\n",
    "# Robust decision layer to use: hellinger or tv\n",
    "dr_layer = 'hellinger'\n",
    "\n",
    "# List of learning rates to test\n",
    "lr_list = [0.005, 0.0125, 0.02]\n",
    "\n",
    "# List of total no. of epochs to test\n",
    "epoch_list = [30, 40, 50, 60, 80, 100]\n",
    "\n",
    "# For replicability, set the random seed for the numerical experiments\n",
    "set_seed = 1000\n",
    "\n",
    "# Load saved models (default is False)\n",
    "use_cache = True\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Run \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "if use_cache:\n",
    "    # Load cached models and backtest results\n",
    "    with open(cache_path+'ew_net.pkl', 'rb') as inp:\n",
    "        ew_net = pickle.load(inp)\n",
    "    with open(cache_path+'po_net.pkl', 'rb') as inp:\n",
    "        po_net = pickle.load(inp)\n",
    "    with open(cache_path+'base_net.pkl', 'rb') as inp:\n",
    "        base_net = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net.pkl', 'rb') as inp:\n",
    "        nom_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net.pkl', 'rb') as inp:\n",
    "        dr_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_po_net.pkl', 'rb') as inp:\n",
    "        dr_po_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'base_net_ext.pkl', 'rb') as inp:\n",
    "        base_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_ext.pkl', 'rb') as inp:\n",
    "        nom_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_ext.pkl', 'rb') as inp:\n",
    "        dr_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta_ext = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'dr_net_tv.pkl', 'rb') as inp:\n",
    "        dr_net_tv = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_theta = pickle.load(inp)\n",
    "else:\n",
    "    # Exp 1: Equal weight portfolio\n",
    "    ew_net = bm.equal_weight(n_x, n_y, n_obs)\n",
    "    ew_net.net_roll_test(X, Y, n_roll=4)\n",
    "    with open(cache_path+'ew_net.pkl', 'wb') as outp:\n",
    "            pickle.dump(ew_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('ew_net run complete')\n",
    "\n",
    "    # Exp 1, 2, 3: Predict-then-optimize system\n",
    "    po_net = bm.pred_then_opt(n_x, n_y, n_obs, set_seed=set_seed, prisk=prisk).double()\n",
    "    po_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'po_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(po_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('po_net run complete')\n",
    "\n",
    "    # Exp 1: Base E2E\n",
    "    base_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='base_mod', perf_loss=perf_loss, \n",
    "                        perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    base_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    base_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'base_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(base_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('base_net run complete')\n",
    "\n",
    "    # Exp 1: Nominal E2E\n",
    "    nom_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net run complete')\n",
    "\n",
    "    # Exp 1: DR E2E\n",
    "    dr_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=True,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net run complete')\n",
    "\n",
    "    # Exp 2: DR predict-then-optimize system\n",
    "    dr_po_net = bm.pred_then_opt(n_x, n_y, n_obs, set_seed=set_seed, prisk=prisk,\n",
    "                                opt_layer=dr_layer).double()\n",
    "    dr_po_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_po_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_po_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_po_net run complete')\n",
    "\n",
    "    # Exp 2: DR E2E (fixed theta and gamma, learn delta)\n",
    "    dr_net_learn_delta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=False, train_delta=True,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_delta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_delta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_delta.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_delta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_delta run complete')\n",
    "\n",
    "    # Exp 3: Nominal E2E (fixed theta, learn gamma)\n",
    "    nom_net_learn_gamma = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_learn_gamma.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net_learn_gamma.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net_learn_gamma.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_learn_gamma, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_learn_gamma run complete')\n",
    "\n",
    "    # Exp 3: DR E2E (fixed theta, learn gamma, fixed delta)\n",
    "    dr_net_learn_gamma = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_gamma.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_gamma.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_gamma.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_gamma, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_gamma run complete')\n",
    "\n",
    "    # Exp 4: Nominal E2E (learn theta, fixed gamma)\n",
    "    nom_net_learn_theta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_learn_theta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net_learn_theta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net_learn_theta.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_learn_theta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_learn_theta run complete')\n",
    "\n",
    "    # Exp 4: DR E2E (learn theta, fixed gamma and delta)\n",
    "    dr_net_learn_theta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_theta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_theta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_theta.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_theta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_theta run complete')\n",
    "\n",
    "####################################################################################################\n",
    "# Merge objects with their extended-epoch counterparts\n",
    "####################################################################################################\n",
    "if use_cache:\n",
    "    portfolios = [\"base_net\", \"nom_net\", \"dr_net\", \"dr_net_learn_delta\", \"nom_net_learn_gamma\",\n",
    "                \"dr_net_learn_gamma\", \"nom_net_learn_theta\", \"dr_net_learn_theta\"]\n",
    "    \n",
    "    for portfolio in portfolios: \n",
    "        cv_combo = pd.concat([eval(portfolio).cv_results, eval(portfolio+'_ext').cv_results], \n",
    "                        ignore_index=True)\n",
    "        eval(portfolio).load_cv_results(cv_combo)\n",
    "        if eval(portfolio).epochs > 50:\n",
    "            exec(portfolio + '=' + portfolio+'_ext')\n",
    "            eval(portfolio).load_cv_results(cv_combo)\n",
    "\n",
    "####################################################################################################\n",
    "# Numerical results\n",
    "####################################################################################################\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 1: General\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net.cv_results = dr_net.cv_results.sort_values(['epochs', 'lr'], ascending=[True, \n",
    "                                                                    True]).reset_index(drop=True)\n",
    "exp1_validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net.cv_results.val_loss.round(4), \n",
    "                            dr_net.cv_results.val_loss.round(4)), axis=1)\n",
    "exp1_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"ew_net\", \"po_net\", \"base_net\", \"nom_net\", \"dr_net\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp1_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['EW', 'PO', 'Base', \n",
    "                                                                    'Nom.', 'DR'])\n",
    "exp1_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n",
    "\n",
    "# Wealth evolution plot\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'EW', r'PO', r'Base', r'Nominal', r'DR']\n",
    "portfolio_list = [ew_net.portfolio, po_net.portfolio, base_net.portfolio, nom_net.portfolio,\n",
    "                dr_net.portfolio]\n",
    "portfolio_colors = [\"dimgray\", \"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp1.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp1.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp1_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                'nom_net':nom_net.gamma_init,\n",
    "                'dr_net':[dr_net.gamma_init, dr_net.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp1_trained_vals = pd.DataFrame(zip([nom_net.gamma_init]+nom_net.gamma_trained, \n",
    "                                    [dr_net.gamma_init]+dr_net.gamma_trained, \n",
    "                                    [dr_net.delta_init]+dr_net.delta_trained), \n",
    "                                    columns=[r'Nom. $\\gamma$', r'DR $\\gamma$', r'DR $\\delta$'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 2: Learn delta\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_delta.cv_results = dr_net_learn_delta.cv_results.sort_values(['epochs', 'lr'],\n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "exp2_validation_table = dr_net_learn_delta.cv_results.round(4)\n",
    "exp2_validation_table.set_axis(['eta', 'Epochs', 'DR (learn delta)'], axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"po_net\", \"dr_po_net\", \"dr_net_learn_delta\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp2_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO','DR','DR (learn delta)'])\n",
    "exp2_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'PO', r'DR', r'DR (learn $\\delta$)']\n",
    "portfolio_list = [po_net.portfolio, dr_po_net.portfolio, dr_net_learn_delta.portfolio]\n",
    "portfolio_colors = [\"forestgreen\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp2.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp2.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp2_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                'dr_po_net':[dr_po_net.gamma.item(), dr_po_net.delta.item()],\n",
    "                'dr_net_learn_delta':[dr_net_learn_delta.gamma_init,dr_net_learn_delta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp2_trained_vals = pd.DataFrame([dr_net_learn_delta.delta_init]+dr_net_learn_delta.delta_trained,\n",
    "                                columns=['DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 3: Learn gamma\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_gamma.cv_results = dr_net_learn_gamma.cv_results.sort_values(['epochs', 'lr'], \n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "dr_net_learn_gamma_delta.cv_results = dr_net_learn_gamma_delta.cv_results.sort_values(['epochs',\n",
    "                                            'lr'], ascending=[True, True]).reset_index(drop=True)\n",
    "exp3_validation_table = pd.concat((nom_net_learn_gamma.cv_results.round(4), \n",
    "                            dr_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma_delta.cv_results.val_loss.round(4)), axis=1)\n",
    "exp3_validation_table.set_axis(['eta', 'Epochs', 'Nom. (learn gamma)', 'DR (learn gamma)', \n",
    "                                'DR (learn gamma + delta)'], axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "# portfolios = [\"po_net\", \"nom_net_learn_gamma\", \"dr_net_learn_gamma\", \"dr_net_learn_gamma_delta\"]\n",
    "portfolios = [\"po_net\", \"nom_net_learn_gamma\", \"dr_net_learn_gamma\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp3_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO', 'Nom. (learn gamma)',\n",
    "                                                                    'DR (learn gamma)'])\n",
    "exp3_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "# portfolio_names = [r'PO', r'Nominal', r'DR ($\\gamma$)', r'DR ($\\gamma + \\delta$)']\n",
    "portfolio_names = [r'PO', r'Nominal', r'DR']\n",
    "portfolio_list = [po_net.portfolio, nom_net_learn_gamma.portfolio, dr_net_learn_gamma.portfolio]\n",
    "portfolio_colors = [\"forestgreen\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp3.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp3.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp3_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "            'nom_net_learn_gamma':nom_net_learn_gamma.gamma_init,\n",
    "            'dr_net_learn_gamma':[dr_net_learn_gamma.gamma_init, dr_net_learn_gamma.delta_init],\n",
    "            'dr_net_learn_gamma_delta':[dr_net_learn_gamma_delta.gamma_init,\n",
    "                                        dr_net_learn_gamma_delta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp3_trained_vals = pd.DataFrame(zip(\n",
    "                    [nom_net_learn_gamma.gamma_init]+nom_net_learn_gamma.gamma_trained, \n",
    "                    [dr_net_learn_gamma.gamma_init]+dr_net_learn_gamma.gamma_trained, \n",
    "                    [dr_net_learn_gamma_delta.gamma_init]+dr_net_learn_gamma_delta.gamma_trained,\n",
    "                    [dr_net_learn_gamma_delta.delta_init]+dr_net_learn_gamma_delta.delta_trained),  \n",
    "                                    columns=['Nom. gamma', 'DR gamma', 'DR gamma 2', 'DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 4: Learn theta\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_theta.cv_results = dr_net_learn_theta.cv_results.sort_values(['epochs', 'lr'], \n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "exp4_validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net_learn_theta.cv_results.val_loss.round(4), \n",
    "                            dr_net_learn_theta.cv_results.val_loss.round(4)), axis=1)\n",
    "exp4_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"po_net\", \"base_net\", \"nom_net_learn_theta\", \"dr_net_learn_theta\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp4_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO', 'Base', 'Nom.', 'DR'])\n",
    "exp4_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'PO', r'Base', r'Nominal', r'DR']\n",
    "portfolio_list = [po_net.portfolio, base_net.portfolio, nom_net_learn_theta.portfolio,\n",
    "                dr_net_learn_theta.portfolio]\n",
    "\n",
    "portfolio_colors = [\"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp4.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp4.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp4_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                    'nom_net_learn_theta':nom_net_learn_theta.gamma_init,\n",
    "                    'dr_net_learn_theta':[dr_net_learn_theta.gamma_init, \n",
    "                                        dr_net_learn_theta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp4_trained_vals = pd.DataFrame(zip(nom_net_learn_theta.gamma_trained, \n",
    "                                    dr_net_learn_theta.gamma_trained, \n",
    "                                    dr_net_learn_theta.delta_trained), \n",
    "                                columns=['Nom. gamma', 'DR gamma', 'DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Aggregate Validation Results\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net.cv_results.val_loss.round(4),\n",
    "                            nom_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            nom_net_learn_theta.cv_results.val_loss.round(4), \n",
    "                            dr_net.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_delta.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma_delta.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_theta.cv_results.val_loss.round(4)), axis=1)\n",
    "validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'Nom. (gamma)', 'Nom. (theta)', \n",
    "                        'DR', 'DR (delta)', 'DR (gamma)', 'DR (gamma+delta)', 'DR (theta)'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "####################################################################################################\n",
    "# Experiment 5 (with synthetic data)\n",
    "####################################################################################################\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "cache_path_exp5 = \"cache/exp5/\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Load data\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Train, validation and test split percentage\n",
    "split = [0.7, 0.3]\n",
    "\n",
    "# Number of feattures and assets\n",
    "n_x, n_y = 5, 10\n",
    "\n",
    "# Number of observations per window and total number of observations\n",
    "n_obs, n_tot = 100, 1200\n",
    "\n",
    "# Synthetic data: randomly generate data from a linear model\n",
    "X, Y = dl.synthetic_exp(n_x=n_x, n_y=n_y, n_obs=n_obs, n_tot=n_tot, split=split)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Performance loss function and performance period 'v+1'\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "\n",
    "# Weight assigned to MSE prediction loss function\n",
    "pred_loss_factor = 0.5\n",
    "\n",
    "# Risk function (default set to variance)\n",
    "prisk = 'p_var'\n",
    "\n",
    "# Robust decision layer to use: hellinger or tv\n",
    "dr_layer = 'hellinger'\n",
    "\n",
    "# Determine whether to train the prediction weights Theta\n",
    "train_pred = True\n",
    "\n",
    "# List of learning rates to test\n",
    "lr_list = [0.005, 0.0125, 0.02]\n",
    "\n",
    "# List of total no. of epochs to test\n",
    "epoch_list = [20, 40, 60]\n",
    "\n",
    "# Load saved models (default is False)\n",
    "use_cache = True\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Run \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "if use_cache:\n",
    "    with open(cache_path_exp5+'nom_net_linear.pkl', 'rb') as inp:\n",
    "        nom_net_linear = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'nom_net_2layer.pkl', 'rb') as inp:\n",
    "        nom_net_2layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'nom_net_3layer.pkl', 'rb') as inp:\n",
    "        nom_net_3layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_linear.pkl', 'rb') as inp:\n",
    "        dr_net_linear = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_2layer.pkl', 'rb') as inp:\n",
    "        dr_net_2layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_3layer.pkl', 'rb') as inp:\n",
    "        dr_net_3layer = pickle.load(inp)\n",
    "else:\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # Linear models\n",
    "    #***********************************************************************************************\n",
    "    \n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 2000\n",
    "\n",
    "    # Nominal E2E linear\n",
    "    nom_net_linear = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, \n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_linear.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_linear.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_linear.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_linear, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_linear run complete')\n",
    "\n",
    "    # DR E2E linear\n",
    "    dr_net_linear = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, \n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_linear.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_linear.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_linear.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_linear, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_linear run complete')\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # 2-layer models\n",
    "    #***********************************************************************************************\n",
    "\n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 3000\n",
    "\n",
    "    # Nominal E2E 2-layer\n",
    "    nom_net_2layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='2layer',\n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_2layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_2layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_2layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_2layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_2layer run complete')\n",
    "\n",
    "    # DR E2E 2-layer\n",
    "    dr_net_2layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='2layer',\n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_2layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_2layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_2layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_2layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_2layer run complete')\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # 3-layer models\n",
    "    #***********************************************************************************************\n",
    "\n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 4000\n",
    "\n",
    "    # Nominal E2E 3-layer\n",
    "    nom_net_3layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='3layer',\n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_3layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_3layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_3layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_3layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_3layer run complete')\n",
    "\n",
    "    # DR E2E 3-layer\n",
    "    dr_net_3layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='3layer',\n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_3layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_3layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_3layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_3layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_3layer run complete')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Results\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "exp5_validation_table = pd.concat((nom_net_linear.cv_results.round(4), \n",
    "                            dr_net_linear.cv_results.val_loss.round(4), \n",
    "                            nom_net_2layer.cv_results.val_loss.round(4), \n",
    "                            dr_net_2layer.cv_results.val_loss.round(4), \n",
    "                            nom_net_3layer.cv_results.val_loss.round(4), \n",
    "                            dr_net_3layer.cv_results.val_loss.round(4)), axis=1)\n",
    "exp5_validation_table.set_axis(['eta', 'Epochs', 'Nom. (linear)', 'DR (linear)', \n",
    "                            'Nom. (2-layer)', 'DR (2-layer)', 'Nom. (3-layer)', 'DR (3-layer)'],\n",
    "                            axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"nom_net_linear\", \"dr_net_linear\", \"nom_net_2layer\", \n",
    "                \"dr_net_2layer\", \"nom_net_3layer\", \"dr_net_3layer\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp5_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['Nom. (linear)', \n",
    "                'DR (linear)', 'Nom. (2-layer)', 'DR (2-layer)', 'Nom. (3-layer)', 'DR (3-layer)'])\n",
    "exp5_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plot\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'Nom. (linear)', r'DR (linear)', r'Nom. (2-layer)', r'DR (2-layer)', \n",
    "                    r'Nom. (3-layer)', r'DR (3-layer)']\n",
    "portfolio_list = [nom_net_linear.portfolio, dr_net_linear.portfolio, nom_net_2layer.portfolio,\n",
    "                dr_net_2layer.portfolio, nom_net_3layer.portfolio, dr_net_3layer.portfolio]\n",
    "portfolio_colors = [\"dodgerblue\", \"salmon\", \"dodgerblue\", \"salmon\", \"dodgerblue\", \"salmon\"]\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, nplots=3,\n",
    "                path=cache_path+\"plots/wealth_exp5.pdf\")\n",
    "\n",
    "from importlib import reload\n",
    "reload(pf)\n",
    "\n",
    "# List of initial parameters\n",
    "exp5_param_dict = dict({'nom_net_linear':nom_net_linear.gamma_init,\n",
    "                    'dr_net_linear':[dr_net_linear.gamma_init, dr_net_linear.delta_init],\n",
    "                    'nom_net_2layer':nom_net_2layer.gamma_init,\n",
    "                    'dr_net_2layer':[dr_net_2layer.gamma_init, dr_net_2layer.delta_init],\n",
    "                    'nom_net_3layer':nom_net_3layer.gamma_init,\n",
    "                    'dr_net_3layer':[dr_net_3layer.gamma_init, dr_net_3layer.delta_init]})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Model Module\n",
    "#\n",
    "####################################################################################################\n",
    "## Import libraries\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import e2edro.RiskFunctions as rf\n",
    "import e2edro.PortfolioClasses as pc\n",
    "import e2edro.e2edro as e2e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "'''\n",
    "Inputs: AssetReturns: Pandas DataFrame, each date is a row, one column for each asset\n",
    "        FactorReturn: Pandas DataFrame, each date is a row, one column for each factor\n",
    "        Technique: Method through which parameters will be estimated. Default: OLS\n",
    "Outputs: mu: numpy array, key: Symbol. value: return estimate\n",
    "         Q: nxn Asset Covariance Matrix (n: # of assets)\n",
    "'''\n",
    "def GetParameterEstimates(AssetReturns, FactorReturns, technique='OLS', log=True, bad=False):\n",
    "    # Only have OLS implemented so far\n",
    "    if technique!='OLS':\n",
    "        return [], []\n",
    "    \n",
    "    if type(AssetReturns) == pd.core.frame.DataFrame:\n",
    "        AssetReturns_np = AssetReturns.to_numpy()\n",
    "        FactorReturns_np = FactorReturns.to_numpy()\n",
    "    else:\n",
    "        AssetReturns_np = AssetReturns.cpu().detach().numpy()[0]\n",
    "        FactorReturns_np = FactorReturns.cpu().detach().numpy()[0][:-1]\n",
    "\n",
    "    if bad:\n",
    "        Q = np.cov(AssetReturns_np, rowvar=False)\n",
    "        mu = 1 - (gmean(1+AssetReturns_np))\n",
    "\n",
    "        return mu, Q\n",
    "\n",
    "    T,n = AssetReturns_np.shape\n",
    "    _, p = FactorReturns_np.shape\n",
    "\n",
    "    # Get Data Matrix - Factors\n",
    "    X = np.zeros((T, p+1))\n",
    "    X[:,:-1] = np.ones((T,1)) # Add ones to first row\n",
    "    X[:,1:] = FactorReturns_np\n",
    "\n",
    "    # Get regression coefficients for Assets\n",
    "    # B = (X^TX)^(-1)X^Ty\n",
    "    B = np.matmul(np.linalg.inv((np.matmul(np.transpose(X), X))), (np.matmul(np.transpose(X), AssetReturns_np)))\n",
    "\n",
    "    # Get alpha and betas\n",
    "    a = np.transpose(B[0,:])\n",
    "    V = B[1:(p+1),:]\n",
    "\n",
    "    # Residual Variance to get D\n",
    "    ep = AssetReturns_np - np.matmul(X, B)\n",
    "    sigma_ep = 1/(T-p-1) * np.sum(np.square(ep), axis=0)\n",
    "    D = np.diag(sigma_ep)\n",
    "\n",
    "    # Get Factor Estimated Return and Covariance Matrix\n",
    "    f_bar = np.transpose(np.mean(FactorReturns_np, axis=0))\n",
    "    F = np.cov(FactorReturns_np, rowvar=False)\n",
    "\n",
    "    # Get mu\n",
    "    mu = a + np.matmul(np.transpose(V), f_bar)\n",
    "\n",
    "    # Get Q\n",
    "    Q = np.matmul(np.matmul(np.transpose(V), F), V) + D\n",
    "\n",
    "    # Make sure Q is PSD\n",
    "    w,v = np.linalg.eig(Q)\n",
    "    min_eig = np.min(w)\n",
    "\n",
    "\n",
    "    if min_eig<0:\n",
    "        print('--Not PSD--Adding Min Eigenvalue--')\n",
    "        Q -= min_eig*np.identity(n)\n",
    "\n",
    "    if log:\n",
    "        print(\"Shape of X: {}\".format(X.shape))\n",
    "        print(\"Shape of B: {}\".format(B.shape))\n",
    "        print(\"Shape of X*B: {}\".format(np.matmul(X, B).shape))\n",
    "        print(\"Shape of ep: {}\".format(ep.shape))\n",
    "        print(\"Shape of sigma_ep: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of D: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of Q: {}\".format(Q.shape))\n",
    "    \n",
    "    return mu, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y.train()) == pd.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    }
   ],
   "source": [
    "mu, Q = GetParameterEstimates(Y.train(), X.train(), log=False, bad=True)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPD(B):\n",
    "    \"\"\"Returns true when input is positive-definite, via Cholesky\"\"\"\n",
    "    try:\n",
    "        _ = np.linalg.cholesky(B)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearestPD(A):\n",
    "    \"\"\"Find the nearest positive-definite matrix to input\n",
    "\n",
    "    A Python/Numpy port of John D'Errico's `nearestSPD` MATLAB code [1], which\n",
    "    credits [2].\n",
    "\n",
    "    [1] https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd\n",
    "\n",
    "    [2] N.J. Higham, \"Computing a nearest symmetric positive semidefinite\n",
    "    matrix\" (1988): https://doi.org/10.1016/0024-3795(88)90223-6\n",
    "    \"\"\"\n",
    "\n",
    "    B = (A + A.T) / 2\n",
    "    _, s, V = np.linalg.svd(B)\n",
    "\n",
    "    H = np.dot(V.T, np.dot(np.diag(s), V))\n",
    "\n",
    "    A2 = (B + H) / 2\n",
    "\n",
    "    A3 = (A2 + A2.T) / 2\n",
    "\n",
    "    if isPD(A3):\n",
    "        return A3\n",
    "\n",
    "    spacing = np.spacing(np.linalg.norm(A))\n",
    "    # The above is different from [1]. It appears that MATLAB's `chol` Cholesky\n",
    "    # decomposition will accept matrixes with exactly 0-eigenvalue, whereas\n",
    "    # Numpy's will not. So where [1] uses `eps(mineig)` (where `eps` is Matlab\n",
    "    # for `np.spacing`), we use the above definition. CAVEAT: our `spacing`\n",
    "    # will be much larger than [1]'s `eps(mineig)`, since `mineig` is usually on\n",
    "    # the order of 1e-16, and `eps(1e-16)` is on the order of 1e-34, whereas\n",
    "    # `spacing` will, for Gaussian random matrixes of small dimension, be on\n",
    "    # othe order of 1e-16. In practice, both ways converge, as the unit test\n",
    "    # below suggests.\n",
    "    I = np.eye(A.shape[0])\n",
    "    k = 1\n",
    "    while not isPD(A3):\n",
    "        mineig = np.min(np.real(np.linalg.eigvals(A3)))\n",
    "        A3 += I * (-mineig * k**2 + spacing)\n",
    "        k += 1\n",
    "\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import math\n",
    "\n",
    "class TraditionalOptimizer:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_y, n_obs):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        n_x: Number of inputs (i.e., features) in the prediction model\n",
    "        n_y: Number of outputs from the prediction model\n",
    "        n_obs: Number of scenarios from which to calculate the sample set of residuals\n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.n_obs = n_obs\n",
    "\n",
    "    '''\n",
    "    Distributionally Robust Risk Parity With Wasserstein Distance Optimizer\n",
    "    Inputs: mu: numpy array, key: Symbol. value: return estimate\n",
    "            Q: nxn Asset Covariance Matrix (n: # of assets)\n",
    "    Outputs: x: optimal allocations\n",
    "\n",
    "    Formula:\n",
    "        \\min_{\\boldsymbol{\\phi} \\in \\mathcal{X}} {(\\sqrt{\\boldsymbol{\\phi}^T \\Sigma_{\\mathcal{P}}(R)\\boldsymbol{\\phi}} + \\sqrt{\\delta}||\\boldsymbol{\\phi}||_p)^2} - c\\sum_{i=1}^n ln(y)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def optimize(self,mu,Q):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_test: Test the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(self, X, Y, n_roll=4):\n",
    "        \"\"\"Neural net rolling window out-of-sample test\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "        n_roll: Number of training periods (i.e., number of times to retrain the model)\n",
    "\n",
    "        Output \n",
    "        self.portfolio: add the backtest results to the e2e_net object\n",
    "        \"\"\"\n",
    "\n",
    "        # Declare backtest object to hold the test results\n",
    "        portfolio = pc.backtest(len(Y.test())-Y.n_obs, self.n_y, Y.test().index[Y.n_obs:])\n",
    "\n",
    "        test_set = DataLoader(pc.SlidingWindow(X.test(), Y.test(), self.n_obs, 0))\n",
    "\n",
    "        # Test model\n",
    "        t = 0\n",
    "        for j, (x, y, y_perf) in enumerate(test_set):\n",
    "            mu, Q = GetParameterEstimates(y, x, log=False, bad=True)\n",
    "            portfolio.weights[t] = self.optimize(mu, Q)\n",
    "            portfolio.rets[t] = y_perf.squeeze() @ portfolio.weights[t]\n",
    "            t += 1\n",
    "\n",
    "        # Calculate the portfolio statistics using the realized portfolio returns\n",
    "        portfolio.stats()\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "class dist_rob_wass(TraditionalOptimizer):\n",
    "    def optimize(self, mu, Q):\n",
    "               \n",
    "        # # of Assets\n",
    "        n = len(mu)\n",
    "\n",
    "        # Decision Variables\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        # Kappa\n",
    "        k = 100\n",
    "\n",
    "        # Size of uncertainty set\n",
    "        delta = 0.23\n",
    "\n",
    "        # Norm for x\n",
    "        p = 2\n",
    "\n",
    "        constraints = [\n",
    "            w>=0 # Disallow Short Sales\n",
    "        ]\n",
    "\n",
    "        # risk = cp.quad_form(w, Q)\n",
    "\n",
    "        log_term = 0\n",
    "        for i in range(n):\n",
    "            log_term += cp.log(w[i])\n",
    "        \n",
    "        # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "        # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "        # To do this, I will take the Cholesky decomposition\n",
    "        # Q = LL^T\n",
    "        # Then, take the 2-norm of L*x\n",
    "\n",
    "        # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "        \n",
    "        Q = nearestPD(Q)\n",
    "        L = np.linalg.cholesky(Q)\n",
    "\n",
    "        obj = cp.power(cp.norm(L@w,2) + delta*cp.norm(w, p),2)\n",
    "        obj = obj - k*log_term\n",
    "\n",
    "        prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "        \n",
    "        # ECOS fails sometimes, if it does then do SCS\n",
    "        try:\n",
    "            prob.solve(verbose=False)\n",
    "        except:\n",
    "            prob.solve(solver='SCS',verbose=False)\n",
    "        \n",
    "        x = w.value\n",
    "        x = np.divide(x, np.sum(x))\n",
    "        \n",
    "        # Check Risk Parity Condition is actually met\n",
    "        # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "        risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class base_risk_parity(TraditionalOptimizer):\n",
    "    def optimize(self, mu, Q):\n",
    "        # # of Assets\n",
    "        n = len(mu)\n",
    "\n",
    "        # Decision Variables\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        # Kappa\n",
    "        k = 2\n",
    "            \n",
    "        constraints = [\n",
    "            w>=0 # Disallow Short Sales\n",
    "        ]\n",
    "\n",
    "        # Objective Function\n",
    "        risk = cp.quad_form(w, Q)\n",
    "        log_term = 0\n",
    "        for i in range(n):\n",
    "            log_term += cp.log(w[i])\n",
    "        \n",
    "        prob = cp.Problem(cp.Minimize(risk-(k*log_term)), constraints=constraints)\n",
    "        \n",
    "        # ECOS fails sometimes, if it does then do SCS\n",
    "        try:\n",
    "            prob.solve(verbose=False)\n",
    "        except:\n",
    "            prob.solve(solver='SCS',verbose=False)\n",
    "        x = w.value\n",
    "        x = np.divide(x, np.sum(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_net = base_risk_parity(n_x, n_y, n_obs)\n",
    "rp_net.net_roll_test(X, Y, n_roll=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaykalim/anaconda3/envs/thesis/lib/python3.10/site-packages/cvxpy/problems/problem.py:1385: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "drrpw_net = dist_rob_wass(n_x, n_y, n_obs)\n",
    "drrpw_net.net_roll_test(X, Y, n_roll=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"cache/exp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/3512328641.py:2: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/3512328641.py:2: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n",
      "/var/folders/by/92cslb_x53v6ccmq5w0pl46r0000gn/T/ipykernel_15704/3512328641.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  dr_net = pickle.load(inp)\n"
     ]
    }
   ],
   "source": [
    "with open(cache_path+'dr_net.pkl', 'rb') as inp:\n",
    "    dr_net = pickle.load(inp)\n",
    "with open(cache_path+'ew_net.pkl', 'rb') as inp:\n",
    "    ew_net = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'drrpw_net' has no attribute 'portfolio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[336], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# dr_net.portfolio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#  'costa'\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m portfolio_list \u001b[39m=\u001b[39m [ew_net\u001b[39m.\u001b[39mportfolio, drrpw_net\u001b[39m.\u001b[39;49mportfolio, rp_net\u001b[39m.\u001b[39mportfolio, drrpw\u001b[39m.\u001b[39mporfolio]\n\u001b[1;32m      4\u001b[0m portfolio_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mEqual Weight\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDRRP-W\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNominal RP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLearn Delta\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m portfolio_colors \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mdimgray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mforestgreen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgoldenrod\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdodgerblue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msalmon\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'drrpw_net' has no attribute 'portfolio'"
     ]
    }
   ],
   "source": [
    "# dr_net.portfolio\n",
    "#  'costa'\n",
    "portfolio_list = [ew_net.portfolio, drrpw_net.portfolio, rp_net.portfolio, drrpw.porfolio]\n",
    "portfolio_names = ['Equal Weight', 'DRRP-W', 'Nominal RP', 'Learn Delta']\n",
    "portfolio_colors = [\"dimgray\", \"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_drrpw_delta_new.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[352], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pf\u001b[39m.\u001b[39;49msr_bar(portfolio_list, portfolio_names, portfolio_colors, \n\u001b[1;32m      2\u001b[0m                 path\u001b[39m=\u001b[39;49mcache_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mplots/sr_bar_drrp_delta0.3.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/thesis/ActualWork/e2e/e2edro/PlotFunctions.py:137\u001b[0m, in \u001b[0;36msr_bar\u001b[0;34m(portfolio_list, names, colors, path)\u001b[0m\n\u001b[1;32m    133\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(portfolio_list)\n\u001b[1;32m    134\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([portfolio_list[i]\u001b[39m.\u001b[39mrets\u001b[39m.\u001b[39mrets\u001b[39m.\u001b[39mrename(names[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \n\u001b[1;32m    135\u001b[0m                     \u001b[39mrange\u001b[39m(n)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m mean_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mexpanding(min_periods\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mgroupby([df\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49myear])\u001b[39m.\u001b[39mtail(\u001b[39m1\u001b[39m)\n\u001b[1;32m    138\u001b[0m std_df  \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mexpanding(min_periods\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstd()\u001b[39m.\u001b[39mgroupby([df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39myear])\u001b[39m.\u001b[39mtail(\u001b[39m1\u001b[39m)\n\u001b[1;32m    139\u001b[0m plot_df \u001b[39m=\u001b[39m mean_df \u001b[39m/\u001b[39m std_df \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m52\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'year'"
     ]
    }
   ],
   "source": [
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_drrp_delta0.3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "## Import libraries\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import e2edro.RiskFunctions as rf\n",
    "import e2edro.LossFunctions as lf\n",
    "import e2edro.PortfolioClasses as pc\n",
    "import e2edro.DataLoad as dl\n",
    "\n",
    "import psutil\n",
    "num_cores = psutil.cpu_count()\n",
    "torch.set_num_threads(num_cores)\n",
    "if psutil.MACOS:\n",
    "    num_cores = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dist_rob_wass(TraditionalOptimizer):\n",
    "    def optimize(self, mu, Q):\n",
    "               \n",
    "        # # of Assets\n",
    "        n = len(mu)\n",
    "\n",
    "        # Decision Variables\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        # Kappa\n",
    "        k = 2\n",
    "\n",
    "        # Size of uncertainty set\n",
    "        delta = 0.05\n",
    "\n",
    "        # Norm for x\n",
    "        p = 2\n",
    "\n",
    "        constraints = [\n",
    "            w>=0 # Disallow Short Sales\n",
    "        ]\n",
    "\n",
    "        # risk = cp.quad_form(w, Q)\n",
    "\n",
    "        log_term = 0\n",
    "        for i in range(n):\n",
    "            log_term += cp.log(w[i])\n",
    "        \n",
    "        # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "        # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "        # To do this, I will take the Cholesky decomposition\n",
    "        # Q = LL^T\n",
    "        # Then, take the 2-norm of L*x\n",
    "\n",
    "        # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "        \n",
    "        Q = nearestPD(Q)\n",
    "        L = np.linalg.cholesky(Q)\n",
    "\n",
    "        obj = cp.power(cp.norm(L@w,2) + math.sqrt(delta)*cp.norm(w, p),2)\n",
    "        obj = obj - k*log_term\n",
    "\n",
    "        prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "        \n",
    "        # ECOS fails sometimes, if it does then do SCS\n",
    "        try:\n",
    "            prob.solve(verbose=False)\n",
    "        except:\n",
    "            prob.solve(solver='SCS',verbose=False)\n",
    "        \n",
    "        x = w.value\n",
    "        x = np.divide(x, np.sum(x))\n",
    "        \n",
    "        # Check Risk Parity Condition is actually met\n",
    "        # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "        risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drrpw_nominal(n_y, n_obs, Q):\n",
    "    \"\"\"Nominal optimization problem declared as a CvxpyLayer object\n",
    "\n",
    "    Inputs\n",
    "    n_y: number of assets\n",
    "    n_obs: Number of scenarios in the dataset\n",
    "    \n",
    "    Variables\n",
    "    w: Decision variable. (n_y x 1) vector of decision variables (e.g., portfolio weights)\n",
    "\n",
    "    Parameters\n",
    "    ep: (n_obs x n_y) matrix of residuals \n",
    "    y_hat: (n_y x 1) vector of predicted outcomes (e.g., conditional expected\n",
    "    returns)\n",
    "    gamma: Scalar. Trade-off between conditional expected return and model error.\n",
    "\n",
    "    Constraints\n",
    "    Total budget is equal to 100%, sum(z) == 1\n",
    "    Long-only positions (no short sales), z >= 0 (specified during the cp.Variable() call)\n",
    "\n",
    "    Objective\n",
    "    Minimize (1/n_obs) * cp.sum(obj_aux) - gamma * mu_aux\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    w = cp.Variable((n_y,1), nonneg=True)\n",
    "    \n",
    "    # Size of uncertainty set\n",
    "    delta = cp.Parameter(nonneg=True)\n",
    "\n",
    "    # Norm for x. TODO set this to be the Mahalanobis Norm\n",
    "    p = 2\n",
    "\n",
    "    # Kappa, dont need this to be trainable as the value of this doesnt really matter\n",
    "    k = 2\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        w>=0.000000001 # Disallow Short Sales\n",
    "    ]\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n_y):\n",
    "        log_term += cp.log(w[i])\n",
    "\n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "\n",
    "    Q = nearestPD(Q)\n",
    "    L = np.linalg.cholesky(Q)\n",
    "\n",
    "    # obj = cp.power(cp.norm(L@w, 2) + delta*cp.norm(w, p),2)\n",
    "    # obj = cp.sum_squares(cp.norm(L@w, 2) + delta*cp.norm(w, p))\n",
    "    # cp.quad_form(w, Q)\n",
    "    # obj = cp.quad_form(w, Q) + 2*delta*cp.norm(w,2)*cp.norm(L@w, 2) + cp.norm(w,2)\n",
    "    obj = cp.quad_form(w, Q) + 2*delta*cp.norm(w,2)*cp.norm(L@w, 2)\n",
    "    obj = obj - k*log_term\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(obj)    \n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem = cp.Problem(objective, constraints=constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[delta], variables=[w])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drrpw_net(nn.Module):\n",
    "    \"\"\"End-to-end Dist. Robust RP with Wasserstein Distance learning neural net module.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_y, n_obs, opt_layer='nominal', prisk='p_var', perf_loss='sharpe_loss',\n",
    "                pred_model='linear', pred_loss_factor=0.5, perf_period=13, train_pred=True, train_gamma=True, train_delta=True, set_seed=None, cache_path='cache/'):\n",
    "        \"\"\"End-to-end learning neural net module\n",
    "\n",
    "        This NN module implements a linear prediction layer 'pred_layer' and a DRO layer \n",
    "        'opt_layer' based on a tractable convex formulation from Ben-Tal et al. (2013). 'delta' and\n",
    "        'gamma' are declared as nn.Parameters so that they can be 'learned'.\n",
    "\n",
    "        Inputs\n",
    "        n_x: Number of inputs (i.e., features) in the prediction model\n",
    "        n_y: Number of outputs from the prediction model\n",
    "        n_obs: Number of scenarios from which to calculate the sample set of residuals\n",
    "        prisk: String. Portfolio risk function. Used in the opt_layer\n",
    "        opt_layer: String. Determines which CvxpyLayer-object to call for the optimization layer\n",
    "        perf_loss: Performance loss function based on out-of-sample financial performance\n",
    "        pred_loss_factor: Trade-off between prediction loss function and performance loss function.\n",
    "            Set 'pred_loss_factor=None' to define the loss function purely as 'perf_loss'\n",
    "        perf_period: Number of lookahead realizations used in 'perf_loss()'\n",
    "        train_pred: Boolean. Choose if the prediction layer is learnable (or keep it fixed)\n",
    "        train_gamma: Boolean. Choose if the risk appetite parameter gamma is learnable\n",
    "        train_delta: Boolean. Choose if the robustness parameter delta is learnable\n",
    "        set_seed: (Optional) Int. Set the random seed for replicability\n",
    "\n",
    "        Output\n",
    "        drrpw_net: nn.Module object \n",
    "        \"\"\"\n",
    "        super(drrpw_net, self).__init__()\n",
    "\n",
    "        # Set random seed (to be used for replicability of numerical experiments)\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed = set_seed\n",
    "\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.n_obs = n_obs\n",
    "\n",
    "        # Prediction loss function\n",
    "        # if pred_loss_factor is not None:\n",
    "        #     self.pred_loss_factor = pred_loss_factor\n",
    "        #     self.pred_loss = torch.nn.MSELoss()\n",
    "        # else:\n",
    "        #     self.pred_loss = None\n",
    "        \n",
    "        self.pred_loss = None\n",
    "\n",
    "        # Define performance loss\n",
    "        self.perf_loss = lf.sharpe_loss\n",
    "\n",
    "        # Number of time steps to evaluate the task loss\n",
    "        self.perf_period = perf_period\n",
    "\n",
    "        # Record the model design: nominal, base or DRO\n",
    "        # Register 'delta' (ambiguity sizing parameter) for DR layer\n",
    "        ub = (1 - 1/n_obs) / 2\n",
    "        lb = (1 - 1/n_obs) / 10\n",
    "        self.delta = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = True\n",
    "        self.delta_init = self.delta.item()\n",
    "        self.model_type = 'dro'\n",
    "\n",
    "        # LAYER: Prediction model\n",
    "        self.pred_model = pred_model\n",
    "        if pred_model == 'linear':\n",
    "            # Linear prediction model\n",
    "            self.pred_layer = nn.Linear(n_x, n_y)\n",
    "            self.pred_layer.weight.requires_grad = train_pred\n",
    "            self.pred_layer.bias.requires_grad = train_pred\n",
    "        \n",
    "        # Store reference path to store model data\n",
    "        self.cache_path = cache_path\n",
    "\n",
    "        # Store initial model\n",
    "        self.init_state_path = cache_path + self.model_type+'_initial_state_' + pred_model\n",
    "        torch.save(self.state_dict(), self.init_state_path)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # forward: forward pass of the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward pass of the NN module\n",
    "\n",
    "        The inputs 'X' are passed through the prediction layer to yield predictions 'Y_hat'. The\n",
    "        residuals from prediction are then calcuclated as 'ep = Y - Y_hat'. Finally, the residuals\n",
    "        are passed to the optimization layer to find the optimal decision z_star.\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "\n",
    "        Other \n",
    "        ep: Residuals. (n_obs x n_y) matrix of the residual between realizations and predictions\n",
    "\n",
    "        Outputs\n",
    "        y_hat: Prediction. (n_y x 1) vector of outputs of the prediction layer\n",
    "        z_star: Optimal solution. (n_y x 1) vector of asset weights\n",
    "        \"\"\"\n",
    "        # Multiple predictions Y_hat from X\n",
    "        Y_hat = torch.stack([self.pred_layer(x_t) for x_t in X])\n",
    "\n",
    "        # Calculate residuals and process them\n",
    "        y_hat = Y_hat[-1]\n",
    "\n",
    "        # Optimization solver arguments (from CVXPY for ECOS/SCS solver)\n",
    "        # solver_args = {'solve_method': 'ECOS', 'max_iters': 2000000, 'abstol': 1e-7}\n",
    "\n",
    "        solver_args = {'solve_method': 'SCS'}\n",
    "\n",
    "        # Covariance Matrix\n",
    "        Q = np.cov(Y.cpu().detach().numpy(), rowvar=False)\n",
    "\n",
    "        # Optimization Layer\n",
    "        self.opt_layer = drrpw_nominal(n_y, n_obs, Q)\n",
    "\n",
    "        # Optimize z per scenario\n",
    "        # Determine whether nominal or dro model\n",
    "        \n",
    "        z_star, _ = self.opt_layer(self.delta, solver_args=solver_args)\n",
    "        # print(z_star)\n",
    "\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        z_star = softmax(z_star)\n",
    "        \n",
    "        # z_star = np.divide(z_star, np.sum(z_star))\n",
    "        \n",
    "        return z_star, y_hat\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_train: Train the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_train(self, train_set, val_set=None, epochs=None, lr=None):\n",
    "        \"\"\"Neural net training module\n",
    "        \n",
    "        Inputs\n",
    "        train_set: SlidingWindow object containing feaatures x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        val_set: SlidingWindow object containing feaatures x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        epochs: Number of training epochs\n",
    "        lr: learning rate\n",
    "\n",
    "        Output\n",
    "        Trained model\n",
    "        (Optional) val_loss: Validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "\n",
    "        # Define the optimizer and its parameters\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Number of elements in training set\n",
    "        n_train = len(train_set)\n",
    "\n",
    "        # Train the neural network\n",
    "        for epoch in range(epochs):\n",
    "                \n",
    "            # TRAINING: forward + backward pass\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for t, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Forward pass: predict and optimize\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Loss function\n",
    "                # print('---z_star---')\n",
    "                # print(z_star)\n",
    "                # print('---y_perf---')\n",
    "                # print(y_perf)\n",
    "                loss = (1/n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                \n",
    "                # Backward pass: backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss of the fully trained model\n",
    "                train_loss += loss.item()\n",
    "        \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ensure that gamma, delta > 0 after taking a descent step\n",
    "            for name, param in self.named_parameters():\n",
    "                if name=='gamma':\n",
    "                    param.data.clamp_(0.0001)\n",
    "                if name=='delta':\n",
    "                    param.data.clamp_(0.0001)\n",
    "\n",
    "        # Compute and return the validation loss of the model\n",
    "        if val_set is not None:\n",
    "\n",
    "            # Number of elements in validation set\n",
    "            n_val = len(val_set)\n",
    "\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "\n",
    "                    # Predict and optimize\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "                \n",
    "                    # Loss function\n",
    "                    loss = (1/n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_cv: Cross validation of the e2e neural net for hyperparameter tuning\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_cv(self, X, Y, lr_list, epoch_list, n_val=4):\n",
    "        \"\"\"Neural net cross-validation module\n",
    "\n",
    "        Inputs\n",
    "        X: Features. TrainTest object of feature timeseries data\n",
    "        Y: Realizations. TrainTest object of asset time series data\n",
    "        epochs: number of training passes\n",
    "        lr_list: List of candidate learning rates\n",
    "        epoch_list: List of candidate number of epochs\n",
    "        n_val: Number of validation folds from the training dataset\n",
    "        \n",
    "        Output\n",
    "        Trained model\n",
    "        \"\"\"\n",
    "        results = pc.CrossVal()\n",
    "        X_temp = dl.TrainTest(X.train(), X.n_obs, [1, 0])\n",
    "        Y_temp = dl.TrainTest(Y.train(), Y.n_obs, [1, 0])\n",
    "        for epochs in epoch_list:\n",
    "            for lr in lr_list:\n",
    "                \n",
    "                # Train the neural network\n",
    "                print('================================================')\n",
    "                print(f\"Training E2E {self.model_type} model: lr={lr}, epochs={epochs}\")\n",
    "                \n",
    "                val_loss_tot = []\n",
    "                for i in range(n_val-1,-1,-1):\n",
    "\n",
    "                    # Partition training dataset into training and validation subset\n",
    "                    split = [round(1-0.2*(i+1),2), 0.2]\n",
    "                    X_temp.split_update(split)\n",
    "                    Y_temp.split_update(split)\n",
    "\n",
    "                    # Construct training and validation DataLoader objects\n",
    "                    train_set = DataLoader(pc.SlidingWindow(X_temp.train(), Y_temp.train(), \n",
    "                                                            self.n_obs, self.perf_period))\n",
    "                    val_set = DataLoader(pc.SlidingWindow(X_temp.test(), Y_temp.test(), \n",
    "                                                            self.n_obs, self.perf_period))\n",
    "\n",
    "                    # Reset learnable parameters gamma and delta\n",
    "                    self.load_state_dict(torch.load(self.init_state_path))\n",
    "\n",
    "                    if self.pred_model == 'linear':\n",
    "                        # Initialize the prediction layer weights to OLS regression weights\n",
    "                        X_train, Y_train = X_temp.train(), Y_temp.train()\n",
    "                        X_train.insert(0,'ones', 1.0)\n",
    "\n",
    "                        X_train = Variable(torch.tensor(X_train.values, dtype=torch.double))\n",
    "                        Y_train = Variable(torch.tensor(Y_train.values, dtype=torch.double))\n",
    "                    \n",
    "                        Theta = torch.inverse(X_train.T @ X_train) @ (X_train.T @ Y_train)\n",
    "                        Theta = Theta.T\n",
    "                        del X_train, Y_train\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            self.pred_layer.bias.copy_(Theta[:,0])\n",
    "                            self.pred_layer.weight.copy_(Theta[:,1:])\n",
    "\n",
    "                    val_loss = self.net_train(train_set, val_set=val_set, lr=lr, epochs=epochs)\n",
    "                    val_loss_tot.append(val_loss)\n",
    "\n",
    "                    print(f\"Fold: {n_val-i} / {n_val}, val_loss: {val_loss}\")\n",
    "\n",
    "                # Store results\n",
    "                results.val_loss.append(np.mean(val_loss_tot))\n",
    "                results.lr.append(lr)\n",
    "                results.epochs.append(epochs)\n",
    "                print('================================================')\n",
    "\n",
    "        # Convert results to dataframe\n",
    "        self.cv_results = results.df()\n",
    "        self.cv_results.to_pickle(self.init_state_path+'_results.pkl')\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = self.cv_results.val_loss.idxmin()\n",
    "        self.lr = self.cv_results.lr[idx]\n",
    "        self.epochs = self.cv_results.epochs[idx]\n",
    "\n",
    "        # Print optimal parameters\n",
    "        print(f\"CV E2E {self.model_type} with hyperparameters: lr={self.lr}, epochs={self.epochs}\")\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(self, X, Y, n_roll=4, lr=None, epochs=None):\n",
    "        \"\"\"Neural net rolling window out-of-sample test\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "        n_roll: Number of training periods (i.e., number of times to retrain the model)\n",
    "        lr: Learning rate for test. If 'None', the optimal learning rate is loaded\n",
    "        epochs: Number of epochs for test. If 'None', the optimal # of epochs is loaded\n",
    "\n",
    "        Output \n",
    "        self.portfolio: add the backtest results to the e2e_net object\n",
    "        \"\"\"\n",
    "\n",
    "        # Declare backtest object to hold the test results\n",
    "        portfolio = pc.backtest(len(Y.test())-Y.n_obs, self.n_y, Y.test().index[Y.n_obs:])\n",
    "\n",
    "        # Store trained gamma and delta values \n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store the squared L2-norm of the prediction weights and their difference from OLS weights\n",
    "        if self.pred_model == 'linear':\n",
    "            self.theta_L2 = []\n",
    "            self.theta_dist_L2 = []\n",
    "\n",
    "        # Store initial train/test split\n",
    "        init_split = Y.split\n",
    "\n",
    "        # Window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll-1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split), Y.split_update(split)\n",
    "            train_set = DataLoader(pc.SlidingWindow(X.train(), Y.train(), self.n_obs, \n",
    "                                                    self.perf_period))\n",
    "            test_set = DataLoader(pc.SlidingWindow(X.test(), Y.test(), self.n_obs, 0))\n",
    "\n",
    "            # Reset learnable parameters gamma and delta\n",
    "            self.load_state_dict(torch.load(self.init_state_path))\n",
    "\n",
    "            if self.pred_model == 'linear':\n",
    "                # Initialize the prediction layer weights to OLS regression weights\n",
    "                X_train, Y_train = X.train(), Y.train()\n",
    "                X_train.insert(0,'ones', 1.0)\n",
    "\n",
    "                X_train = Variable(torch.tensor(X_train.values, dtype=torch.double))\n",
    "                Y_train = Variable(torch.tensor(Y_train.values, dtype=torch.double))\n",
    "            \n",
    "                Theta = torch.inverse(X_train.T @ X_train) @ (X_train.T @ Y_train)\n",
    "                Theta = Theta.T\n",
    "                del X_train, Y_train\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.pred_layer.bias.copy_(Theta[:,0])\n",
    "                    self.pred_layer.weight.copy_(Theta[:,1:])\n",
    "\n",
    "            # Train model using all available data preceding the test window\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "\n",
    "            # Store trained values of gamma and delta\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "\n",
    "            # Store the squared L2 norm of theta and distance between theta and OLS weights\n",
    "            if self.pred_model == 'linear':\n",
    "                theta_L2 = (torch.sum(self.pred_layer.weight**2, axis=()) + \n",
    "                            torch.sum(self.pred_layer.bias**2, axis=()))\n",
    "                theta_dist_L2 = (torch.sum((self.pred_layer.weight - Theta[:,1:])**2, axis=()) + \n",
    "                                torch.sum((self.pred_layer.bias - Theta[:,0])**2, axis=()))\n",
    "                self.theta_L2.append(theta_L2)\n",
    "                self.theta_dist_L2.append(theta_dist_L2)\n",
    "\n",
    "            # Test model\n",
    "            with torch.no_grad():\n",
    "                for j, (x, y, y_perf) in enumerate(test_set):\n",
    "                \n",
    "                    # Predict and optimize\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Store portfolio weights and returns for each time step 't'\n",
    "                    portfolio.weights[t] = z_star.squeeze()\n",
    "                    portfolio.rets[t] = y_perf.squeeze() @ portfolio.weights[t]\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset\n",
    "        X, Y = X.split_update(init_split), Y.split_update(init_split)\n",
    "\n",
    "        # Calculate the portfolio statistics using the realized portfolio returns\n",
    "        portfolio.stats()\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross validation results\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        \"\"\"Load cross validation results\n",
    "\n",
    "        Inputs\n",
    "        cv_results: pd.dataframe containing the cross validation results\n",
    "\n",
    "        Outputs\n",
    "        self.lr: Load the optimal learning rate\n",
    "        self.epochs: Load the optimal number of epochs\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the cross validation results within the object\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999789293"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drrpw_nominal(n_y, n_obs, Q):\n",
    "    # Variables\n",
    "    phi = cp.Variable((n_y,1), nonneg=True)\n",
    "    t = cp.Variable()\n",
    "    \n",
    "    # Size of uncertainty set\n",
    "    delta = cp.Parameter(nonneg=True)\n",
    "\n",
    "    # Norm for x. TODO set this to be the Mahalanobis Norm\n",
    "    p = 2\n",
    "\n",
    "    # Kappa, dont need this to be trainable as the value of this doesnt really matter\n",
    "    k = 2\n",
    "\n",
    "    # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "    # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "    # To do this, I will take the Cholesky decomposition\n",
    "    # Q = LL^T\n",
    "    # Then, take the 2-norm of L*x\n",
    "\n",
    "    # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "\n",
    "    Q = nearestPD(Q)\n",
    "    L = np.linalg.cholesky(Q)\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        phi >= 0.000000001,\n",
    "        t >= cp.power(cp.norm(L@phi, 2) + delta*cp.norm(phi, p),2)\n",
    "    ]\n",
    "\n",
    "    log_term = 0\n",
    "    for i in range(n_y):\n",
    "        log_term += cp.log(phi[i])\n",
    "\n",
    "\n",
    "    # obj = cp.power(cp.norm(L@w, 2) + delta*cp.norm(w, p),2)\n",
    "    # obj = cp.sum_squares(cp.norm(L@w, 2) + delta*cp.norm(w, p))\n",
    "    # cp.quad_form(w, Q)\n",
    "    # obj = cp.quad_form(w, Q) + 2*delta*cp.norm(w,2)*cp.norm(L@w, 2) + cp.norm(w,2)\n",
    "    # print('using this one')\n",
    "    # obj = 2*delta*cp.norm(w,2)*cp.norm(L@w_tilde, 2)\n",
    "    obj = (t) - k*log_term\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(obj)    \n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem = cp.Problem(objective, constraints=constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[delta], variables=[phi, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Training E2E dro model: lr=0.005, epochs=3\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "Fold: 1 / 1, val_loss: -0.29129537749655593\n",
      "================================================\n",
      "================================================\n",
      "Training E2E dro model: lr=0.0125, epochs=3\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "Fold: 1 / 1, val_loss: -0.29137932500159336\n",
      "================================================\n",
      "================================================\n",
      "Training E2E dro model: lr=0.02, epochs=3\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "Fold: 1 / 1, val_loss: -0.2914690494462127\n",
      "================================================\n",
      "CV E2E dro with hyperparameters: lr=0.02, epochs=3\n",
      "Out-of-sample window: 1 / 1\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "--loss--\n",
      "nom_net_linear run complete\n"
     ]
    }
   ],
   "source": [
    "# Nominal E2E linear\n",
    "drrpw = drrpw_net(n_x, n_y, n_obs, prisk=prisk, train_pred=True, \n",
    "                train_gamma=True, train_delta=True, \n",
    "                set_seed=set_seed, opt_layer='nominal', \n",
    "                perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "drrpw.net_cv(X, Y, lr_list, [3], n_val=1)\n",
    "drrpw.net_roll_test(X, Y, n_roll=1)\n",
    "with open(cache_path+'drrpw.pkl', 'wb') as outp:\n",
    "    pickle.dump(drrpw, outp, pickle.HIGHEST_PROTOCOL)\n",
    "print('nom_net_linear run complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------\n",
    "def wealth_plot(portfolio_list, names, colors, nplots=1, path=None):\n",
    "    \"\"\"Plot of the portfolio wealth evolution over time (also known as the 'Total Return Index')\n",
    "\n",
    "    Inputs\n",
    "    portfolio_list: list of portfolio objects corresponding to the backtest of each model\n",
    "    names: list of strings with the portfolio names that shall appear in the plot legend\n",
    "    colors: list of strings with matplotlib color names to be used for each portfolio\n",
    "    nplots: Number of subplots into which to distribute the results\n",
    "    path: Path to which to save the image in pdf format. If 'None', then the image is not saved\n",
    "\n",
    "    Output\n",
    "    Wealth evolution figure\n",
    "    \"\"\"\n",
    "    n = len(portfolio_list)\n",
    "    plot_df = pd.concat([portfolio_list[i].rets.tri.rename(names[i])*100 for i in \n",
    "                        range(n)], axis=1)\n",
    "    s = pd.DataFrame([100*np.ones(n)], columns=names)\n",
    "    print(plot_df)\n",
    "    if isinstance(plot_df.index, pd.DatetimeIndex):\n",
    "        s.index = [plot_df.index[0] - pd.Timedelta(days=7)]\n",
    "    else:\n",
    "        s.index = [plot_df.index[0] - 1]\n",
    "    plot_df = pd.concat([s, plot_df])\n",
    "\n",
    "    if nplots == 1:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        for i in range(n):\n",
    "            ax.plot(plot_df[names[i]], color=colors[i])\n",
    "        ax.legend(names, ncol=n, fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "                handlelength=1)\n",
    "        # ax.legend(names, fontsize=14)\n",
    "        ax.grid(b=\"on\",linestyle=\":\",linewidth=0.8)\n",
    "        ax.tick_params(axis='x', labelrotation = 30)\n",
    "        plt.ylabel(\"Total wealth\", fontsize=14)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(max([6, nplots*4]),4), ncols=nplots)\n",
    "        for i in range(n):\n",
    "            j = int(nplots/n * i)\n",
    "            ax[j].plot(plot_df[names[i]], color=colors[i])\n",
    "            if j == 0:\n",
    "                ax[j].set_ylabel(\"Total wealth\", fontsize=14)\n",
    "            ax[j].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "        for j in range(nplots):\n",
    "            i = int(j * n / nplots)\n",
    "            k = int((j+1) * n / nplots)\n",
    "            ax[j].legend(names[i:k], ncol=int(n / nplots), fontsize=12, loc='upper center', \n",
    "                        bbox_to_anchor=(0.5, -0.15), handlelength=1)\n",
    "            # ax[j].legend(names[i:k], fontsize=14)\n",
    "            ax[j].grid(visible=\"on\",linestyle=\":\",linewidth=0.8)\n",
    "            ax[j].tick_params(axis='x', labelrotation = 30)\n",
    "\n",
    "    if path is not None:\n",
    "        fig.savefig(path, bbox_inches='tight')\n",
    "        fig.savefig(path[0:-3]+'ps', bbox_inches='tight', format='ps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Equal Weight  Nominal RP  Learn Delta\n",
      "Date                                                      \n",
      "2013-01-25 00:00:00    100.779611  100.499415          NaN\n",
      "2013-02-01 00:00:00    101.789115  101.609329          NaN\n",
      "2013-02-08 00:00:00    101.891921  101.776992          NaN\n",
      "2013-02-15 00:00:00    102.059295  101.597010          NaN\n",
      "2013-02-22 00:00:00    101.330709  101.628334          NaN\n",
      "...                           ...         ...          ...\n",
      "1195                          NaN         NaN   140.812263\n",
      "1196                          NaN         NaN   141.169202\n",
      "1197                          NaN         NaN   141.085792\n",
      "1198                          NaN         NaN   141.128165\n",
      "1199                          NaN         NaN   141.842763\n",
      "\n",
      "[814 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[347], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m portfolio_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mEqual Weight\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNominal RP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLearn Delta\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m portfolio_colors \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mdimgray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mforestgreen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgoldenrod\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdodgerblue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msalmon\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n\u001b[1;32m      7\u001b[0m                 path\u001b[39m=\u001b[39;49mcache_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mplots/wealth_drrpw_delta_new.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[346], line 23\u001b[0m, in \u001b[0;36mwealth_plot\u001b[0;34m(portfolio_list, names, colors, nplots, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     s\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m [plot_df\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m pd\u001b[39m.\u001b[39mTimedelta(days\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m)]\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     s\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m [plot_df\u001b[39m.\u001b[39;49mindex[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m]\n\u001b[1;32m     24\u001b[0m plot_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([s, plot_df])\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m nplots \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/_libs/tslibs/timestamps.pyx:530\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__sub__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/_libs/tslibs/timestamps.pyx:499\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`"
     ]
    }
   ],
   "source": [
    "# dr_net.portfolio\n",
    "#  'costa'\n",
    "portfolio_list = [ew_net.portfolio, rp_net.portfolio, drrpw.portfolio]\n",
    "portfolio_names = ['Equal Weight', 'Nominal RP', 'Learn Delta']\n",
    "portfolio_colors = [\"dimgray\", \"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_drrpw_delta_new.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Date'>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drrpw.portfolio.rets.tri.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<e2edro.PortfolioClasses.backtest at 0x16047fac0>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rp_net.portfolio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

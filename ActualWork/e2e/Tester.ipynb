{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ew_net run complete\n",
      "Out-of-sample window: 1 / 4\n",
      "Out-of-sample window: 2 / 4\n",
      "Out-of-sample window: 3 / 4\n",
      "Out-of-sample window: 4 / 4\n",
      "po_net run complete\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=30\n",
      "Fold: 1 / 4, val_loss: -0.15367348983933962\n",
      "Fold: 2 / 4, val_loss: -0.2306497254484821\n",
      "Fold: 3 / 4, val_loss: -0.06689603580647087\n",
      "Fold: 4 / 4, val_loss: 0.03233659587386935\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.0125, epochs=30\n",
      "Fold: 1 / 4, val_loss: -0.1769424319429289\n",
      "Fold: 2 / 4, val_loss: -0.036153658367926465\n",
      "Fold: 3 / 4, val_loss: -0.08860059902357909\n",
      "Fold: 4 / 4, val_loss: -0.16950788477475687\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.02, epochs=30\n",
      "Fold: 1 / 4, val_loss: -0.16645916436480737\n",
      "Fold: 2 / 4, val_loss: -0.04975001169916195\n",
      "Fold: 3 / 4, val_loss: -0.01809634438742508\n",
      "Fold: 4 / 4, val_loss: -0.2453040672410111\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=40\n",
      "Fold: 1 / 4, val_loss: -0.18670907515269336\n",
      "Fold: 2 / 4, val_loss: -0.2407380237454455\n",
      "Fold: 3 / 4, val_loss: -0.08469245677898386\n",
      "Fold: 4 / 4, val_loss: 0.026910512768349763\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.0125, epochs=40\n",
      "Fold: 1 / 4, val_loss: -0.17671822737134527\n",
      "Fold: 2 / 4, val_loss: -0.00113344793317177\n",
      "Fold: 3 / 4, val_loss: -0.09465396391819704\n",
      "Fold: 4 / 4, val_loss: -0.16709845204705193\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.02, epochs=40\n",
      "Fold: 1 / 4, val_loss: -0.14549725166634553\n",
      "Fold: 2 / 4, val_loss: -0.04974518168423878\n",
      "Fold: 3 / 4, val_loss: 0.019626706469735522\n",
      "Fold: 4 / 4, val_loss: -0.24530956740082507\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=50\n",
      "Fold: 1 / 4, val_loss: -0.22013844368912538\n",
      "Fold: 2 / 4, val_loss: -0.2587673145644017\n",
      "Fold: 3 / 4, val_loss: -0.15284841785323156\n",
      "Fold: 4 / 4, val_loss: 0.03838231439463959\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.0125, epochs=50\n",
      "Fold: 1 / 4, val_loss: -0.16914651603136774\n",
      "Fold: 2 / 4, val_loss: -0.016181762959014086\n",
      "Fold: 3 / 4, val_loss: -0.09465101657034397\n",
      "Fold: 4 / 4, val_loss: -0.06735024261893993\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.02, epochs=50\n",
      "Fold: 1 / 4, val_loss: -0.174086436684417\n",
      "Fold: 2 / 4, val_loss: -0.04974332016904501\n",
      "Fold: 3 / 4, val_loss: 0.019630702355329165\n",
      "Fold: 4 / 4, val_loss: -0.24504165017269577\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=60\n",
      "Fold: 1 / 4, val_loss: -0.22771042317701884\n",
      "Fold: 2 / 4, val_loss: -0.26769148424648986\n",
      "Fold: 3 / 4, val_loss: -0.10848913202449599\n",
      "Fold: 4 / 4, val_loss: 0.046021871835126515\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.0125, epochs=60\n",
      "Fold: 1 / 4, val_loss: -0.1702948459636628\n",
      "Fold: 2 / 4, val_loss: -0.05690283234265239\n",
      "Fold: 3 / 4, val_loss: -0.0946498966765381\n",
      "Fold: 4 / 4, val_loss: -0.06733278364416785\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.02, epochs=60\n",
      "Fold: 1 / 4, val_loss: -0.17667131414597012\n",
      "Fold: 2 / 4, val_loss: -0.049742540322954006\n",
      "Fold: 3 / 4, val_loss: 0.019625309385145204\n",
      "Fold: 4 / 4, val_loss: -0.225543756559829\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=80\n",
      "Fold: 1 / 4, val_loss: -0.22622713185611212\n",
      "Fold: 2 / 4, val_loss: -0.26863683970101326\n",
      "Fold: 3 / 4, val_loss: -0.15304381078586324\n",
      "Fold: 4 / 4, val_loss: 0.04602255707793653\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.0125, epochs=80\n",
      "Fold: 1 / 4, val_loss: -0.14868183698824827\n",
      "Fold: 2 / 4, val_loss: -0.15768475488797984\n",
      "Fold: 3 / 4, val_loss: -0.0946492738002778\n",
      "Fold: 4 / 4, val_loss: -0.06732020753749067\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.02, epochs=80\n",
      "Fold: 1 / 4, val_loss: -0.18931361549726974\n",
      "Fold: 2 / 4, val_loss: -0.049742148861234084\n",
      "Fold: 3 / 4, val_loss: 0.022865053506794235\n",
      "Fold: 4 / 4, val_loss: -0.06732948413062673\n",
      "================================================\n",
      "================================================\n",
      "Training E2E base_mod model: lr=0.005, epochs=100\n",
      "Fold: 1 / 4, val_loss: -0.21668006106836246\n",
      "Fold: 2 / 4, val_loss: -0.268636917015965\n",
      "Fold: 3 / 4, val_loss: -0.15913638352133636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12128\\1352522585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    162\u001b[0m                         \u001b[0mset_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'base_mod'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperf_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperf_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                         perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[0mbase_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[0mbase_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_roll_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'base_net.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rafay\\Documents\\thesis\\ActualWork\\Backtester\\E2E-DRO\\e2edro\\e2edro.py\u001b[0m in \u001b[0;36mnet_cv\u001b[1;34m(self, X, Y, lr_list, epoch_list, n_val)\u001b[0m\n\u001b[0;32m    579\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m                     \u001b[0mval_loss_tot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rafay\\Documents\\thesis\\ActualWork\\Backtester\\E2E-DRO\\e2edro\\e2edro.py\u001b[0m in \u001b[0;36mnet_train\u001b[1;34m(self, train_set, val_set, epochs, lr)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m                 \u001b[1;31m# Accumulate loss of the fully trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Distributionally Robust End-to-End Portfolio Construction\n",
    "# Experiment 1 - General\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Make the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Import E2E_DRO functions\n",
    "from e2edro import e2edro as e2e\n",
    "from e2edro import DataLoad as dl\n",
    "from e2edro import BaseModels as bm\n",
    "from e2edro import PlotFunctions as pf\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "# cache_path = \"./cache/exp/\"\n",
    "cache_path = \"C:\\\\Users\\\\Rafay\\Documents\\\\thesis\\\\ActualWork\\\\Backtester\\\\E2E-DRO\\\\cache\\\\exp\\\\\"\n",
    "####################################################################################################\n",
    "# Experiments 1-4 (with hisotrical data): Load data\n",
    "####################################################################################################\n",
    "\n",
    "# Data frequency and start/end dates\n",
    "freq = 'weekly'\n",
    "start = '2000-01-01'\n",
    "end = '2021-09-30'\n",
    "\n",
    "# Train, validation and test split percentage\n",
    "split = [0.6, 0.4]\n",
    "\n",
    "# Number of observations per window \n",
    "n_obs = 104\n",
    "\n",
    "# Number of assets\n",
    "n_y = 20\n",
    "\n",
    "# AlphaVantage API Key. \n",
    "# Note: User API keys can be obtained for free from www.alphavantage.co. Users will need a free \n",
    "# academic or paid license to download adjusted closing pricing data from AlphaVantage.\n",
    "AV_key = 'W5ACAYR6PEX7L28T'\n",
    "\n",
    "# Historical data: Download data (or load cached data)\n",
    "X, Y = dl.AV(start, end, split, freq=freq, n_obs=n_obs, n_y=n_y, use_cache=True,\n",
    "            save_results=False, AV_key=AV_key)\n",
    "\n",
    "# Number of features and assets\n",
    "n_x, n_y = X.data.shape[1], Y.data.shape[1]\n",
    "\n",
    "####################################################################################################\n",
    "# E2E Learning System Run\n",
    "####################################################################################################\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Performance loss function and performance period 'v+1'\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "\n",
    "# Weight assigned to MSE prediction loss function\n",
    "pred_loss_factor = 0.5\n",
    "\n",
    "# Risk function (default set to variance)\n",
    "prisk = 'p_var'\n",
    "\n",
    "# Robust decision layer to use: hellinger or tv\n",
    "dr_layer = 'hellinger'\n",
    "\n",
    "# List of learning rates to test\n",
    "lr_list = [0.005, 0.0125, 0.02]\n",
    "\n",
    "# List of total no. of epochs to test\n",
    "epoch_list = [30, 40, 50, 60, 80, 100]\n",
    "\n",
    "# For replicability, set the random seed for the numerical experiments\n",
    "set_seed = 1000\n",
    "\n",
    "# Load saved models (default is False)\n",
    "use_cache = False\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Run \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "if use_cache:\n",
    "    # Load cached models and backtest results\n",
    "    with open(cache_path+'ew_net.pkl', 'rb') as inp:\n",
    "        ew_net = pickle.load(inp)\n",
    "    with open(cache_path+'po_net.pkl', 'rb') as inp:\n",
    "        po_net = pickle.load(inp)\n",
    "    with open(cache_path+'base_net.pkl', 'rb') as inp:\n",
    "        base_net = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net.pkl', 'rb') as inp:\n",
    "        nom_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net.pkl', 'rb') as inp:\n",
    "        dr_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_po_net.pkl', 'rb') as inp:\n",
    "        dr_po_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'base_net_ext.pkl', 'rb') as inp:\n",
    "        base_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_ext.pkl', 'rb') as inp:\n",
    "        nom_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_ext.pkl', 'rb') as inp:\n",
    "        dr_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta_ext = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'dr_net_tv.pkl', 'rb') as inp:\n",
    "        dr_net_tv = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_theta = pickle.load(inp)\n",
    "else:\n",
    "    # Exp 1: Equal weight portfolio\n",
    "    ew_net = bm.equal_weight(n_x, n_y, n_obs)\n",
    "    ew_net.net_roll_test(X, Y, n_roll=4)\n",
    "    with open(cache_path+'ew_net.pkl', 'wb') as outp:\n",
    "            pickle.dump(ew_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('ew_net run complete')\n",
    "\n",
    "    # Exp 1, 2, 3: Predict-then-optimize system\n",
    "    po_net = bm.pred_then_opt(n_x, n_y, n_obs, set_seed=set_seed, prisk=prisk).double()\n",
    "    po_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'po_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(po_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('po_net run complete')\n",
    "\n",
    "    # Exp 1: Base E2E\n",
    "    base_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='base_mod', perf_loss=perf_loss, \n",
    "                        perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    base_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    base_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'base_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(base_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('base_net run complete')\n",
    "\n",
    "    # Exp 1: Nominal E2E\n",
    "    nom_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net run complete')\n",
    "\n",
    "    # Exp 1: DR E2E\n",
    "    dr_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=True,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net run complete')\n",
    "\n",
    "    # Exp 2: DR predict-then-optimize system\n",
    "    dr_po_net = bm.pred_then_opt(n_x, n_y, n_obs, set_seed=set_seed, prisk=prisk,\n",
    "                                opt_layer=dr_layer).double()\n",
    "    dr_po_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_po_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_po_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_po_net run complete')\n",
    "\n",
    "    # Exp 2: DR E2E (fixed theta and gamma, learn delta)\n",
    "    dr_net_learn_delta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=False, train_delta=True,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_delta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_delta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_delta.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_delta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_delta run complete')\n",
    "\n",
    "    # Exp 3: Nominal E2E (fixed theta, learn gamma)\n",
    "    nom_net_learn_gamma = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_learn_gamma.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net_learn_gamma.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net_learn_gamma.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_learn_gamma, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_learn_gamma run complete')\n",
    "\n",
    "    # Exp 3: DR E2E (fixed theta, learn gamma, fixed delta)\n",
    "    dr_net_learn_gamma = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=False, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_gamma.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_gamma.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_gamma.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_gamma, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_gamma run complete')\n",
    "\n",
    "    # Exp 4: Nominal E2E (learn theta, fixed gamma)\n",
    "    nom_net_learn_theta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_learn_theta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net_learn_theta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net_learn_theta.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_learn_theta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_learn_theta run complete')\n",
    "\n",
    "    # Exp 4: DR E2E (learn theta, fixed gamma and delta)\n",
    "    dr_net_learn_theta = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_learn_theta.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net_learn_theta.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net_learn_theta.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_learn_theta, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_learn_theta run complete')\n",
    "\n",
    "####################################################################################################\n",
    "# Merge objects with their extended-epoch counterparts\n",
    "####################################################################################################\n",
    "if use_cache:\n",
    "    portfolios = [\"base_net\", \"nom_net\", \"dr_net\", \"dr_net_learn_delta\", \"nom_net_learn_gamma\",\n",
    "                \"dr_net_learn_gamma\", \"nom_net_learn_theta\", \"dr_net_learn_theta\"]\n",
    "    \n",
    "    for portfolio in portfolios: \n",
    "        cv_combo = pd.concat([eval(portfolio).cv_results, eval(portfolio+'_ext').cv_results], \n",
    "                        ignore_index=True)\n",
    "        eval(portfolio).load_cv_results(cv_combo)\n",
    "        if eval(portfolio).epochs > 50:\n",
    "            exec(portfolio + '=' + portfolio+'_ext')\n",
    "            eval(portfolio).load_cv_results(cv_combo)\n",
    "\n",
    "####################################################################################################\n",
    "# Numerical results\n",
    "####################################################################################################\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 1: General\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net.cv_results = dr_net.cv_results.sort_values(['epochs', 'lr'], ascending=[True, \n",
    "                                                                    True]).reset_index(drop=True)\n",
    "exp1_validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net.cv_results.val_loss.round(4), \n",
    "                            dr_net.cv_results.val_loss.round(4)), axis=1)\n",
    "exp1_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"ew_net\", \"po_net\", \"base_net\", \"nom_net\", \"dr_net\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp1_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['EW', 'PO', 'Base', \n",
    "                                                                    'Nom.', 'DR'])\n",
    "exp1_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True)\n",
    "\n",
    "# Wealth evolution plot\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'EW', r'PO', r'Base', r'Nominal', r'DR']\n",
    "portfolio_list = [ew_net.portfolio, po_net.portfolio, base_net.portfolio, nom_net.portfolio,\n",
    "                dr_net.portfolio]\n",
    "portfolio_colors = [\"dimgray\", \"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp1.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp1.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp1_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                'nom_net':nom_net.gamma_init,\n",
    "                'dr_net':[dr_net.gamma_init, dr_net.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp1_trained_vals = pd.DataFrame(zip([nom_net.gamma_init]+nom_net.gamma_trained, \n",
    "                                    [dr_net.gamma_init]+dr_net.gamma_trained, \n",
    "                                    [dr_net.delta_init]+dr_net.delta_trained), \n",
    "                                    columns=[r'Nom. $\\gamma$', r'DR $\\gamma$', r'DR $\\delta$'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 2: Learn delta\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_delta.cv_results = dr_net_learn_delta.cv_results.sort_values(['epochs', 'lr'],\n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "exp2_validation_table = dr_net_learn_delta.cv_results.round(4)\n",
    "exp2_validation_table.set_axis(['eta', 'Epochs', 'DR (learn delta)'], axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"po_net\", \"dr_po_net\", \"dr_net_learn_delta\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp2_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO','DR','DR (learn delta)'])\n",
    "exp2_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'PO', r'DR', r'DR (learn $\\delta$)']\n",
    "portfolio_list = [po_net.portfolio, dr_po_net.portfolio, dr_net_learn_delta.portfolio]\n",
    "portfolio_colors = [\"forestgreen\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp2.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp2.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp2_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                'dr_po_net':[dr_po_net.gamma.item(), dr_po_net.delta.item()],\n",
    "                'dr_net_learn_delta':[dr_net_learn_delta.gamma_init,dr_net_learn_delta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp2_trained_vals = pd.DataFrame([dr_net_learn_delta.delta_init]+dr_net_learn_delta.delta_trained,\n",
    "                                columns=['DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 3: Learn gamma\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_gamma.cv_results = dr_net_learn_gamma.cv_results.sort_values(['epochs', 'lr'], \n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "dr_net_learn_gamma_delta.cv_results = dr_net_learn_gamma_delta.cv_results.sort_values(['epochs',\n",
    "                                            'lr'], ascending=[True, True]).reset_index(drop=True)\n",
    "exp3_validation_table = pd.concat((nom_net_learn_gamma.cv_results.round(4), \n",
    "                            dr_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma_delta.cv_results.val_loss.round(4)), axis=1)\n",
    "exp3_validation_table.set_axis(['eta', 'Epochs', 'Nom. (learn gamma)', 'DR (learn gamma)', \n",
    "                                'DR (learn gamma + delta)'], axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "# portfolios = [\"po_net\", \"nom_net_learn_gamma\", \"dr_net_learn_gamma\", \"dr_net_learn_gamma_delta\"]\n",
    "portfolios = [\"po_net\", \"nom_net_learn_gamma\", \"dr_net_learn_gamma\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp3_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO', 'Nom. (learn gamma)',\n",
    "                                                                    'DR (learn gamma)'])\n",
    "exp3_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "# portfolio_names = [r'PO', r'Nominal', r'DR ($\\gamma$)', r'DR ($\\gamma + \\delta$)']\n",
    "portfolio_names = [r'PO', r'Nominal', r'DR']\n",
    "portfolio_list = [po_net.portfolio, nom_net_learn_gamma.portfolio, dr_net_learn_gamma.portfolio]\n",
    "portfolio_colors = [\"forestgreen\", \"dodgerblue\", \"salmon\"]\n",
    "\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp3.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp3.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp3_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "            'nom_net_learn_gamma':nom_net_learn_gamma.gamma_init,\n",
    "            'dr_net_learn_gamma':[dr_net_learn_gamma.gamma_init, dr_net_learn_gamma.delta_init],\n",
    "            'dr_net_learn_gamma_delta':[dr_net_learn_gamma_delta.gamma_init,\n",
    "                                        dr_net_learn_gamma_delta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp3_trained_vals = pd.DataFrame(zip(\n",
    "                    [nom_net_learn_gamma.gamma_init]+nom_net_learn_gamma.gamma_trained, \n",
    "                    [dr_net_learn_gamma.gamma_init]+dr_net_learn_gamma.gamma_trained, \n",
    "                    [dr_net_learn_gamma_delta.gamma_init]+dr_net_learn_gamma_delta.gamma_trained,\n",
    "                    [dr_net_learn_gamma_delta.delta_init]+dr_net_learn_gamma_delta.delta_trained),  \n",
    "                                    columns=['Nom. gamma', 'DR gamma', 'DR gamma 2', 'DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 4: Learn theta\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "dr_net_learn_theta.cv_results = dr_net_learn_theta.cv_results.sort_values(['epochs', 'lr'], \n",
    "                                                    ascending=[True, True]).reset_index(drop=True)\n",
    "exp4_validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net_learn_theta.cv_results.val_loss.round(4), \n",
    "                            dr_net_learn_theta.cv_results.val_loss.round(4)), axis=1)\n",
    "exp4_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"po_net\", \"base_net\", \"nom_net_learn_theta\", \"dr_net_learn_theta\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp4_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['PO', 'Base', 'Nom.', 'DR'])\n",
    "exp4_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'PO', r'Base', r'Nominal', r'DR']\n",
    "portfolio_list = [po_net.portfolio, base_net.portfolio, nom_net_learn_theta.portfolio,\n",
    "                dr_net_learn_theta.portfolio]\n",
    "\n",
    "portfolio_colors = [\"forestgreen\", \"goldenrod\", \"dodgerblue\", \"salmon\"]\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/wealth_exp4.pdf\")\n",
    "pf.sr_bar(portfolio_list, portfolio_names, portfolio_colors, \n",
    "                path=cache_path+\"plots/sr_bar_exp4.pdf\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp4_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                    'nom_net_learn_theta':nom_net_learn_theta.gamma_init,\n",
    "                    'dr_net_learn_theta':[dr_net_learn_theta.gamma_init, \n",
    "                                        dr_net_learn_theta.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp4_trained_vals = pd.DataFrame(zip(nom_net_learn_theta.gamma_trained, \n",
    "                                    dr_net_learn_theta.gamma_trained, \n",
    "                                    dr_net_learn_theta.delta_trained), \n",
    "                                columns=['Nom. gamma', 'DR gamma', 'DR delta'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Aggregate Validation Results\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "validation_table = pd.concat((base_net.cv_results.round(4), \n",
    "                            nom_net.cv_results.val_loss.round(4),\n",
    "                            nom_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            nom_net_learn_theta.cv_results.val_loss.round(4), \n",
    "                            dr_net.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_delta.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_gamma_delta.cv_results.val_loss.round(4),\n",
    "                            dr_net_learn_theta.cv_results.val_loss.round(4)), axis=1)\n",
    "validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'Nom. (gamma)', 'Nom. (theta)', \n",
    "                        'DR', 'DR (delta)', 'DR (gamma)', 'DR (gamma+delta)', 'DR (theta)'], \n",
    "                        axis=1, inplace=True) \n",
    "\n",
    "####################################################################################################\n",
    "# Experiment 5 (with synthetic data)\n",
    "####################################################################################################\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "cache_path_exp5 = \"C:\\\\Users\\\\Rafay\\Documents\\\\thesis\\\\ActualWork\\\\Backtester\\\\E2E-DRO\\\\cache\\\\exp5\\\\\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Load data\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Train, validation and test split percentage\n",
    "split = [0.7, 0.3]\n",
    "\n",
    "# Number of feattures and assets\n",
    "n_x, n_y = 5, 10\n",
    "\n",
    "# Number of observations per window and total number of observations\n",
    "n_obs, n_tot = 100, 1200\n",
    "\n",
    "# Synthetic data: randomly generate data from a linear model\n",
    "X, Y = dl.synthetic_exp(n_x=n_x, n_y=n_y, n_obs=n_obs, n_tot=n_tot, split=split)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Performance loss function and performance period 'v+1'\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "\n",
    "# Weight assigned to MSE prediction loss function\n",
    "pred_loss_factor = 0.5\n",
    "\n",
    "# Risk function (default set to variance)\n",
    "prisk = 'p_var'\n",
    "\n",
    "# Robust decision layer to use: hellinger or tv\n",
    "dr_layer = 'hellinger'\n",
    "\n",
    "# Determine whether to train the prediction weights Theta\n",
    "train_pred = True\n",
    "\n",
    "# List of learning rates to test\n",
    "lr_list = [0.005, 0.0125, 0.02]\n",
    "\n",
    "# List of total no. of epochs to test\n",
    "epoch_list = [20, 40, 60]\n",
    "\n",
    "# Load saved models (default is False)\n",
    "use_cache = True\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Run \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "if use_cache:\n",
    "    with open(cache_path_exp5+'nom_net_linear.pkl', 'rb') as inp:\n",
    "        nom_net_linear = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'nom_net_2layer.pkl', 'rb') as inp:\n",
    "        nom_net_2layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'nom_net_3layer.pkl', 'rb') as inp:\n",
    "        nom_net_3layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_linear.pkl', 'rb') as inp:\n",
    "        dr_net_linear = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_2layer.pkl', 'rb') as inp:\n",
    "        dr_net_2layer = pickle.load(inp)\n",
    "    with open(cache_path_exp5+'dr_net_3layer.pkl', 'rb') as inp:\n",
    "        dr_net_3layer = pickle.load(inp)\n",
    "else:\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # Linear models\n",
    "    #***********************************************************************************************\n",
    "    \n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 2000\n",
    "\n",
    "    # Nominal E2E linear\n",
    "    nom_net_linear = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, \n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_linear.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_linear.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_linear.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_linear, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_linear run complete')\n",
    "\n",
    "    # DR E2E linear\n",
    "    dr_net_linear = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, \n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_linear.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_linear.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_linear.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_linear, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_linear run complete')\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # 2-layer models\n",
    "    #***********************************************************************************************\n",
    "\n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 3000\n",
    "\n",
    "    # Nominal E2E 2-layer\n",
    "    nom_net_2layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='2layer',\n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_2layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_2layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_2layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_2layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_2layer run complete')\n",
    "\n",
    "    # DR E2E 2-layer\n",
    "    dr_net_2layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='2layer',\n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_2layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_2layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_2layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_2layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_2layer run complete')\n",
    "\n",
    "    #***********************************************************************************************\n",
    "    # 3-layer models\n",
    "    #***********************************************************************************************\n",
    "\n",
    "    # For replicability, set the random seed for the numerical experiments\n",
    "    set_seed = 4000\n",
    "\n",
    "    # Nominal E2E 3-layer\n",
    "    nom_net_3layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='3layer',\n",
    "                    set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net_3layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    nom_net_3layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'nom_net_3layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net_3layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net_3layer run complete')\n",
    "\n",
    "    # DR E2E 3-layer\n",
    "    dr_net_3layer = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk, train_pred=train_pred, \n",
    "                    train_gamma=True, train_delta=True, pred_model='3layer',\n",
    "                    set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss, \n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net_3layer.net_cv(X, Y, lr_list, epoch_list, n_val=1)\n",
    "    dr_net_3layer.net_roll_test(X, Y, n_roll=1)\n",
    "    with open(cache_path+'dr_net_3layer.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net_3layer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net_3layer run complete')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Experiment 5: Results\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Validation results table\n",
    "exp5_validation_table = pd.concat((nom_net_linear.cv_results.round(4), \n",
    "                            dr_net_linear.cv_results.val_loss.round(4), \n",
    "                            nom_net_2layer.cv_results.val_loss.round(4), \n",
    "                            dr_net_2layer.cv_results.val_loss.round(4), \n",
    "                            nom_net_3layer.cv_results.val_loss.round(4), \n",
    "                            dr_net_3layer.cv_results.val_loss.round(4)), axis=1)\n",
    "exp5_validation_table.set_axis(['eta', 'Epochs', 'Nom. (linear)', 'DR (linear)', \n",
    "                            'Nom. (2-layer)', 'DR (2-layer)', 'Nom. (3-layer)', 'DR (3-layer)'],\n",
    "                            axis=1, inplace=True) \n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "portfolios = [\"nom_net_linear\", \"dr_net_linear\", \"nom_net_2layer\", \n",
    "                \"dr_net_2layer\", \"nom_net_3layer\", \"dr_net_3layer\"]\n",
    "rets =[]\n",
    "vols = []\n",
    "SRs = []\n",
    "for portfolio in portfolios:\n",
    "    ret = (eval(portfolio).portfolio.rets.tri.iloc[-1] ** \n",
    "            (1/eval(portfolio).portfolio.rets.tri.shape[0]))**52 - 1\n",
    "    vol = eval(portfolio).portfolio.vol * np.sqrt(52)\n",
    "    SR = ret / vol\n",
    "    rets.append(round(ret*100, ndigits=1))\n",
    "    vols.append(round(vol*100, ndigits=1))\n",
    "    SRs.append(round(SR, ndigits=2))\n",
    "\n",
    "exp5_fin_table  = pd.DataFrame(np.array([rets, vols, SRs]), columns=['Nom. (linear)', \n",
    "                'DR (linear)', 'Nom. (2-layer)', 'DR (2-layer)', 'Nom. (3-layer)', 'DR (3-layer)'])\n",
    "exp5_fin_table.set_axis(['Return (%)', 'Volatility (%)', 'Sharpe ratio'], axis=0, inplace=True) \n",
    "\n",
    "# Wealth evolution plot\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'Nom. (linear)', r'DR (linear)', r'Nom. (2-layer)', r'DR (2-layer)', \n",
    "                    r'Nom. (3-layer)', r'DR (3-layer)']\n",
    "portfolio_list = [nom_net_linear.portfolio, dr_net_linear.portfolio, nom_net_2layer.portfolio,\n",
    "                dr_net_2layer.portfolio, nom_net_3layer.portfolio, dr_net_3layer.portfolio]\n",
    "portfolio_colors = [\"dodgerblue\", \"salmon\", \"dodgerblue\", \"salmon\", \"dodgerblue\", \"salmon\"]\n",
    "pf.wealth_plot(portfolio_list, portfolio_names, portfolio_colors, nplots=3,\n",
    "                path=cache_path+\"plots/wealth_exp5.pdf\")\n",
    "\n",
    "from importlib import reload\n",
    "reload(pf)\n",
    "\n",
    "# List of initial parameters\n",
    "exp5_param_dict = dict({'nom_net_linear':nom_net_linear.gamma_init,\n",
    "                    'dr_net_linear':[dr_net_linear.gamma_init, dr_net_linear.delta_init],\n",
    "                    'nom_net_2layer':nom_net_2layer.gamma_init,\n",
    "                    'dr_net_2layer':[dr_net_2layer.gamma_init, dr_net_2layer.delta_init],\n",
    "                    'nom_net_3layer':nom_net_3layer.gamma_init,\n",
    "                    'dr_net_3layer':[dr_net_3layer.gamma_init, dr_net_3layer.delta_init]})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Model Module\n",
    "#\n",
    "####################################################################################################\n",
    "## Import libraries\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import e2edro.RiskFunctions as rf\n",
    "import e2edro.PortfolioClasses as pc\n",
    "import e2edro.e2edro as e2e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs: AssetReturns: Pandas DataFrame, each date is a row, one column for each asset\n",
    "        FactorReturn: Pandas DataFrame, each date is a row, one column for each factor\n",
    "        Technique: Method through which parameters will be estimated. Default: OLS\n",
    "Outputs: mu: numpy array, key: Symbol. value: return estimate\n",
    "         Q: nxn Asset Covariance Matrix (n: # of assets)\n",
    "'''\n",
    "def GetParameterEstimates(AssetReturns, FactorReturns, technique='OLS', log=True):\n",
    "    print(AssetReturns)\n",
    "    # Only have OLS implemented so far\n",
    "    if technique!='OLS':\n",
    "        return [], []\n",
    "    \n",
    "    AssetReturns_np = AssetReturns.to_numpy()\n",
    "    FactorReturns_np = FactorReturns.to_numpy()\n",
    "    T,n = AssetReturns_np.shape\n",
    "    _, p = FactorReturns.shape\n",
    "\n",
    "    # Get Data Matrix - Factors\n",
    "    X = np.zeros((T, p+1))\n",
    "    X[:,:-1] = np.ones((T,1)) # Add ones to first row\n",
    "    X[:,1:] = FactorReturns\n",
    "\n",
    "    # Get regression coefficients for Assets\n",
    "    # B = (X^TX)^(-1)X^Ty\n",
    "    B = np.matmul(np.linalg.inv((np.matmul(np.transpose(X), X))), (np.matmul(np.transpose(X), AssetReturns_np)))\n",
    "\n",
    "    # Get alpha and betas\n",
    "    a = np.transpose(B[0,:])\n",
    "    V = B[1:(p+1),:]\n",
    "\n",
    "    # Residual Variance to get D\n",
    "    ep = AssetReturns_np - np.matmul(X, B)\n",
    "    sigma_ep = 1/(T-p-1) * np.sum(np.square(ep), axis=0)\n",
    "    D = np.diag(sigma_ep)\n",
    "\n",
    "    # Get Factor Estimated Return and Covariance Matrix\n",
    "    f_bar = np.transpose(np.mean(FactorReturns_np, axis=0))\n",
    "    F = np.cov(FactorReturns_np, rowvar=False)\n",
    "\n",
    "    # Get mu\n",
    "    mu = a + np.matmul(np.transpose(V), f_bar)\n",
    "\n",
    "    # Get Q\n",
    "    Q = np.matmul(np.matmul(np.transpose(V), F), V) + D\n",
    "\n",
    "    # Make sure Q is PSD\n",
    "    w,v = np.linalg.eig(Q)\n",
    "    min_eig = np.min(w)\n",
    "\n",
    "\n",
    "    if min_eig<0:\n",
    "        Q -= min_eig*np.identity(n)\n",
    "\n",
    "    if log:\n",
    "        print(\"Shape of X: {}\".format(X.shape))\n",
    "        print(\"Shape of B: {}\".format(B.shape))\n",
    "        print(\"Shape of X*B: {}\".format(np.matmul(X, B).shape))\n",
    "        print(\"Shape of ep: {}\".format(ep.shape))\n",
    "        print(\"Shape of sigma_ep: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of D: {}\".format(sigma_ep.shape))\n",
    "        print(\"Shape of Q: {}\".format(Q.shape))\n",
    "    \n",
    "    return mu, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "mu, Q = GetParameterEstimates(Y.train(), X.train(), log=False)\n",
    "print(mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
      "date                                                                     \n",
      "2011-01-28  0.028710 -0.009600 -0.035396 -0.034765 -0.016560 -0.045614   \n",
      "2011-02-04  0.030943  0.000685  0.027989  0.021186  0.001123  0.050735   \n",
      "2011-02-11  0.029870 -0.018725  0.075712  0.012448  0.044405  0.033590   \n",
      "2011-02-18 -0.017626 -0.001079 -0.014531  0.006148  0.030706 -0.001354   \n",
      "2011-02-25 -0.006846 -0.018847 -0.049651 -0.042770 -0.027500 -0.037288   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2021-09-03  0.038358  0.004738  0.038339 -0.024935 -0.021834 -0.028944   \n",
      "2021-09-10 -0.034543 -0.018031 -0.002559 -0.022481 -0.013355 -0.019001   \n",
      "2021-09-17 -0.019534  0.014068 -0.001911  0.005606  0.002034  0.005711   \n",
      "2021-09-24  0.005888 -0.001734 -0.010686  0.017439  0.033993  0.040494   \n",
      "2021-10-01 -0.036891 -0.058226 -0.041010 -0.014049  0.003987  0.007356   \n",
      "\n",
      "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
      "date                                                                     \n",
      "2011-01-28  0.000127  0.119673 -0.023064  0.017405 -0.011543  0.031590   \n",
      "2011-02-04  0.054311  0.046490  0.010508 -0.011817  0.030585  0.040865   \n",
      "2011-02-11 -0.000250 -0.027875  0.028224 -0.006068  0.015055  0.039663   \n",
      "2011-02-18  0.020285  0.077733 -0.000131 -0.005567  0.005197  0.022407   \n",
      "2011-02-25  0.009941 -0.022449 -0.014186 -0.065547 -0.024659 -0.036463   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2021-09-03 -0.016138 -0.016281  0.011106  0.018632  0.027113 -0.011559   \n",
      "2021-09-10 -0.016220 -0.031730  0.001507 -0.022513  0.005643 -0.025479   \n",
      "2021-09-17  0.021860  0.037890  0.013839 -0.007951 -0.012146 -0.025657   \n",
      "2021-09-24  0.044054  0.036014  0.016207 -0.010779  0.017932 -0.014668   \n",
      "2021-10-01  0.021358  0.029524 -0.021549 -0.026472 -0.039337 -0.024642   \n",
      "\n",
      "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
      "date                                                                     \n",
      "2011-01-28 -0.012876 -0.042292 -0.011384 -0.022396  0.019456 -0.029651   \n",
      "2011-02-04  0.032097  0.013831  0.074578  0.047876  0.019085  0.017461   \n",
      "2011-02-11  0.012142 -0.002301 -0.024482  0.066323  0.002203  0.017876   \n",
      "2011-02-18  0.001469  0.006755  0.019254  0.003455  0.006320  0.003512   \n",
      "2011-02-25 -0.011612 -0.015218 -0.017196 -0.014004 -0.017750 -0.015401   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2021-09-03 -0.009441  0.012201  0.005150  0.004774  0.012050  0.015850   \n",
      "2021-09-10 -0.028427 -0.046104 -0.026687  0.017238 -0.021649 -0.013788   \n",
      "2021-09-17 -0.014167 -0.013296 -0.037289 -0.003530  0.000922  0.012877   \n",
      "2021-09-24  0.025045 -0.002367  0.001139 -0.040715  0.001658 -0.014530   \n",
      "2021-10-01 -0.012646 -0.017401 -0.021165 -0.038807 -0.006621 -0.004423   \n",
      "\n",
      "                  ED       NEM  \n",
      "date                            \n",
      "2011-01-28  0.004010 -0.015043  \n",
      "2011-02-04 -0.009587  0.035818  \n",
      "2011-02-11  0.010284 -0.001580  \n",
      "2011-02-18 -0.009659  0.028481  \n",
      "2011-02-25  0.007956 -0.069060  \n",
      "...              ...       ...  \n",
      "2021-09-03  0.008726  0.025434  \n",
      "2021-09-10 -0.021101 -0.035788  \n",
      "2021-09-17 -0.021690 -0.031058  \n",
      "2021-09-24 -0.000958 -0.024448  \n",
      "2021-10-01 -0.005616  0.007982  \n",
      "\n",
      "[558 rows x 20 columns]\n",
      "tensor([[[-1.2406e-02, -3.1176e-02,  2.4330e-03,  1.7686e-02,  1.1764e-03,\n",
      "          -1.8414e-02,  9.9532e-03, -7.3944e-03],\n",
      "         [-3.7144e-03,  7.0984e-03, -8.1307e-04, -1.4102e-03, -8.1338e-04,\n",
      "           1.1484e-02,  2.0865e-03, -1.8429e-03],\n",
      "         [ 2.7689e-02,  4.1937e-03, -7.4120e-03, -7.9809e-03,  4.8059e-03,\n",
      "           1.2344e-02, -3.7124e-03,  1.0708e-02],\n",
      "         [ 1.6674e-02,  1.0533e-02,  7.2908e-03, -4.0100e-03,  6.7106e-03,\n",
      "           1.5879e-02, -4.1865e-04,  1.2543e-02],\n",
      "         [ 1.0917e-02,  5.6934e-03,  5.9066e-03, -3.2072e-03,  3.8984e-03,\n",
      "          -2.3260e-04, -6.0191e-03,  5.5840e-03],\n",
      "         [-1.7297e-02,  2.6065e-04,  7.7134e-05, -3.7495e-03, -4.7190e-03,\n",
      "           1.3564e-04,  4.1739e-03, -1.2418e-02],\n",
      "         [ 1.7871e-03,  3.1752e-03, -6.8983e-03,  2.8333e-04, -5.4029e-03,\n",
      "           1.2218e-02, -2.7113e-03, -1.0873e-02],\n",
      "         [-1.4205e-02, -1.4168e-02,  9.3074e-03,  1.0911e-02, -5.4127e-03,\n",
      "          -2.0868e-02,  6.9037e-03,  4.5688e-03],\n",
      "         [-1.8139e-02,  8.1651e-03,  9.7071e-04,  2.6926e-03,  6.4126e-03,\n",
      "           6.1841e-03, -3.3089e-03, -7.4903e-03],\n",
      "         [ 2.9437e-02,  1.2120e-02, -1.1653e-02, -3.1059e-03,  3.1961e-03,\n",
      "           2.4889e-02,  8.7234e-04, -1.0061e-03],\n",
      "         [ 1.5947e-02,  1.2447e-02, -5.0000e-03,  5.2037e-03,  4.9703e-04,\n",
      "           8.6161e-03, -3.2968e-03, -2.7043e-03],\n",
      "         [-2.9118e-03, -2.1298e-03, -1.7278e-03,  3.6010e-03,  8.9020e-04,\n",
      "          -1.1211e-02,  5.3429e-03, -2.5244e-03],\n",
      "         [-6.8299e-03, -3.2570e-03, -1.1169e-02,  7.5072e-03, -4.7998e-03,\n",
      "           1.4138e-03,  4.0472e-03, -6.7850e-03],\n",
      "         [ 1.3491e-02,  8.6782e-04, -1.1457e-02, -1.0097e-03,  1.0939e-03,\n",
      "           1.4739e-02,  9.8199e-05, -4.4013e-03],\n",
      "         [ 2.0128e-02,  5.9701e-04, -1.9027e-03, -1.8054e-03, -5.9041e-03,\n",
      "          -3.0297e-03,  3.3844e-03, -7.3893e-03],\n",
      "         [-1.8711e-02, -1.8439e-02, -3.6112e-03,  1.5472e-02, -5.6968e-03,\n",
      "          -2.5435e-02, -1.1947e-02, -4.3947e-03],\n",
      "         [-5.6394e-04,  5.1330e-03, -1.3940e-02,  9.9225e-03, -8.6745e-03,\n",
      "          -1.7328e-03, -1.7110e-02, -1.6997e-02],\n",
      "         [-3.7036e-03, -7.0513e-03, -1.1723e-04, -1.9095e-03, -5.0680e-04,\n",
      "           6.2100e-03,  4.3794e-03, -4.1972e-03],\n",
      "         [ 1.7957e-04,  1.0868e-02, -2.6063e-03, -2.5131e-03, -6.7381e-06,\n",
      "           1.5817e-02,  1.6546e-02, -2.0937e-04],\n",
      "         [-2.4974e-02, -8.7084e-03, -7.0582e-04,  1.7702e-03, -8.0907e-03,\n",
      "          -8.0780e-04, -6.6319e-03, -4.9042e-03],\n",
      "         [-2.3722e-02, -8.7960e-03,  6.8525e-04,  9.7167e-03, -6.2910e-03,\n",
      "          -5.6527e-03, -6.2254e-03, -1.4282e-02],\n",
      "         [-4.5132e-05,  1.4357e-03,  9.9985e-03,  1.2154e-02, -4.0137e-03,\n",
      "          -1.1836e-02,  7.1846e-03,  2.8816e-03],\n",
      "         [ 2.1900e-03,  2.3081e-02, -1.2168e-02,  1.7929e-03, -1.0054e-03,\n",
      "           2.0712e-02,  1.1654e-02, -5.1156e-03],\n",
      "         [ 5.5787e-02, -3.2916e-04,  1.6003e-04, -2.6065e-03,  5.0944e-03,\n",
      "           2.1642e-02,  1.7810e-02,  1.0827e-02],\n",
      "         [ 5.2291e-03,  1.0232e-02, -1.6820e-02,  8.7179e-03, -9.6725e-03,\n",
      "           1.7356e-02, -2.0287e-06, -9.1109e-03],\n",
      "         [-2.1521e-02, -2.7440e-03, -4.1221e-03,  1.0922e-02, -4.0178e-03,\n",
      "          -9.3956e-04, -5.4328e-03, -1.4939e-02],\n",
      "         [ 2.0636e-02, -6.8160e-03,  9.5573e-04,  1.2667e-03, -5.5294e-03,\n",
      "          -8.5106e-04, -1.3811e-03,  6.4157e-04],\n",
      "         [-4.1218e-02, -1.2471e-02,  9.0273e-03,  5.5903e-03, -1.0183e-03,\n",
      "          -1.0778e-02,  8.8937e-04, -2.8033e-03],\n",
      "         [-7.7540e-02, -2.2847e-02, -3.1293e-03,  2.5296e-02, -3.9983e-03,\n",
      "          -2.7362e-02, -3.1581e-02, -3.5463e-02],\n",
      "         [-1.6060e-02, -1.2309e-02, -3.2107e-02,  1.0772e-02, -1.3038e-02,\n",
      "           3.6787e-02,  2.1199e-02, -3.9521e-02],\n",
      "         [-5.0523e-02, -1.4585e-02,  1.6645e-02,  6.0890e-03,  1.6503e-02,\n",
      "          -4.0522e-02, -4.1202e-02, -1.1158e-02],\n",
      "         [ 5.0101e-02,  1.0108e-02, -1.1618e-02, -9.3204e-04, -6.5255e-03,\n",
      "           2.6728e-02,  3.3227e-02,  1.6938e-03],\n",
      "         [-2.1294e-03, -9.1977e-03, -9.0019e-03,  5.7967e-04, -5.0877e-04,\n",
      "           1.4142e-02,  1.2709e-02, -1.6652e-03],\n",
      "         [-1.5688e-02,  2.7209e-03, -2.7888e-03, -3.7886e-03, -2.3007e-03,\n",
      "           5.9293e-03, -6.8077e-03, -1.2095e-03],\n",
      "         [ 5.4711e-02,  1.8460e-03, -2.0429e-03, -7.6907e-03, -5.4901e-03,\n",
      "          -5.3518e-04,  8.0732e-03,  1.4051e-02],\n",
      "         [-6.7769e-02, -1.6126e-02, -1.9193e-02,  2.1939e-02,  2.3763e-03,\n",
      "           2.6267e-02, -1.6848e-02, -3.2365e-02],\n",
      "         [-6.1246e-03, -7.3077e-03,  2.3445e-02, -3.5862e-03,  7.4959e-03,\n",
      "          -1.8087e-02, -7.7526e-03, -7.8342e-03],\n",
      "         [ 2.2770e-02,  1.9637e-03, -1.5051e-02,  5.8816e-03, -4.0013e-03,\n",
      "          -2.0148e-03,  3.0953e-02, -1.1502e-03],\n",
      "         [ 6.2809e-02,  2.0556e-02, -1.3505e-03, -8.8066e-03, -1.1069e-02,\n",
      "          -1.0390e-02,  3.5765e-02,  1.3532e-02],\n",
      "         [ 9.5231e-03, -1.3648e-02,  2.2877e-02, -9.5951e-03,  1.0532e-02,\n",
      "          -1.6651e-02, -4.9184e-03,  1.1128e-02],\n",
      "         [ 4.0884e-02,  2.4772e-02,  4.5080e-03, -2.1167e-02, -4.5110e-03,\n",
      "          -4.0687e-02, -1.1912e-02,  2.5267e-02],\n",
      "         [-2.2822e-02,  8.5665e-03, -1.8996e-02,  1.5288e-02, -4.0135e-04,\n",
      "           2.5148e-02,  2.0548e-02, -2.5282e-02],\n",
      "         [ 6.1742e-03, -8.6323e-03,  6.3566e-03,  1.1002e-02,  8.8149e-03,\n",
      "           8.7496e-03,  9.1985e-03, -2.1660e-03],\n",
      "         [-3.6836e-02,  4.8599e-03, -2.4811e-03,  9.6313e-03,  6.0874e-03,\n",
      "           2.6836e-02,  3.5625e-03, -9.8968e-03],\n",
      "         [-4.8636e-02, -2.5563e-02, -3.8223e-03,  6.6960e-03,  2.7921e-03,\n",
      "           2.5306e-02, -1.6062e-03, -1.7023e-02],\n",
      "         [ 7.6880e-02,  2.1200e-02,  5.8871e-03, -1.5096e-02, -1.0267e-03,\n",
      "          -2.1418e-02,  4.9302e-02,  1.6719e-02],\n",
      "         [ 8.4574e-03,  5.1905e-03,  1.6155e-03,  3.2272e-04, -3.2001e-04,\n",
      "          -2.3266e-03,  4.8600e-03,  4.0030e-03],\n",
      "         [-2.8737e-02, -2.9759e-03,  5.9627e-03,  1.2143e-02,  1.5675e-02,\n",
      "           2.9808e-02, -2.5964e-02,  6.6566e-04],\n",
      "         [ 3.6626e-02, -3.6541e-03,  1.4568e-02, -7.1715e-03,  6.4740e-03,\n",
      "          -1.0055e-02,  4.6023e-03,  1.4583e-02],\n",
      "         [-6.1398e-03, -7.3855e-04, -5.3074e-03,  4.0950e-03,  1.5947e-03,\n",
      "           5.3864e-03,  5.0939e-03, -5.4109e-03],\n",
      "         [ 1.7022e-02, -5.7048e-03,  8.8908e-03, -8.5033e-03, -1.9004e-03,\n",
      "          -3.1163e-02,  9.7792e-03,  1.0593e-02],\n",
      "         [ 1.1698e-02,  8.5177e-03,  2.0800e-03, -1.3993e-02,  6.3133e-03,\n",
      "          -2.2467e-02,  1.6924e-03,  1.2829e-02],\n",
      "         [ 2.1331e-02,  5.8948e-03, -1.1528e-03, -2.1410e-03, -5.6927e-03,\n",
      "          -2.6798e-02, -5.9157e-03, -4.6593e-04],\n",
      "         [ 3.4473e-03,  1.6062e-02, -1.5232e-02,  1.2756e-03, -9.9634e-03,\n",
      "          -1.0972e-02, -4.6116e-03, -1.2085e-03],\n",
      "         [ 2.5018e-02,  1.5929e-02,  4.9923e-03, -1.2777e-02, -3.0710e-04,\n",
      "          -1.8318e-02, -1.4700e-02,  1.1694e-02],\n",
      "         [-2.4310e-03, -1.6200e-02, -2.2160e-03,  4.8010e-03, -3.0319e-04,\n",
      "           8.1624e-03,  1.5747e-03, -7.2389e-04],\n",
      "         [ 1.5903e-02,  3.5513e-03,  4.8869e-03, -1.2301e-04, -7.6670e-06,\n",
      "          -7.2510e-03,  5.4821e-03,  1.9813e-03],\n",
      "         [ 2.6791e-03, -5.8609e-03, -1.0787e-02,  3.6834e-03, -4.7005e-03,\n",
      "           7.5066e-03, -1.2006e-03, -1.1679e-02],\n",
      "         [ 4.4902e-04, -3.1644e-02,  4.2998e-03,  4.8074e-03,  6.8115e-03,\n",
      "           1.0619e-02, -7.1875e-03,  2.5997e-03],\n",
      "         [ 3.1637e-03,  1.7178e-02,  1.1822e-03, -4.2755e-04,  2.6946e-03,\n",
      "           1.4147e-02,  3.5997e-03,  4.5706e-03],\n",
      "         [ 2.2963e-02, -6.7086e-03,  1.5957e-02, -1.0312e-02,  1.5968e-03,\n",
      "          -2.0598e-02, -1.0414e-03,  2.4084e-02],\n",
      "         [-4.3422e-03,  6.1651e-03, -2.5261e-03,  2.9791e-03, -3.7023e-03,\n",
      "           1.2078e-02, -1.1015e-02,  1.8447e-04],\n",
      "         [ 7.3999e-03, -6.5221e-03, -3.5177e-03,  1.0955e-03,  3.6959e-03,\n",
      "           9.4213e-03, -2.5521e-03, -2.6135e-03],\n",
      "         [-7.1716e-03, -6.8071e-03, -6.7899e-03,  8.5264e-03,  2.8557e-04,\n",
      "           2.7445e-02, -2.2163e-03, -1.0459e-02],\n",
      "         [-2.0452e-02, -2.1505e-03, -1.9359e-03,  6.6611e-03,  2.7988e-03,\n",
      "           5.9006e-03,  3.5227e-03, -5.4719e-03],\n",
      "         [ 5.5700e-03,  2.3761e-03,  3.1560e-03,  1.8815e-03,  9.0011e-03,\n",
      "           7.2288e-03, -4.2537e-03,  8.2824e-05],\n",
      "         [ 1.8788e-02,  7.9843e-03, -2.3899e-03, -3.9170e-03, -4.3449e-03,\n",
      "           3.2741e-03,  2.2725e-02, -3.8413e-03],\n",
      "         [-2.6833e-02, -1.5866e-02, -5.3325e-04,  9.0205e-03,  6.0084e-03,\n",
      "           1.5386e-02, -1.0374e-02, -4.8954e-03],\n",
      "         [-9.8870e-03,  8.2249e-03,  2.0749e-04, -3.8016e-03,  8.6173e-03,\n",
      "           1.6181e-02, -1.7395e-02, -6.3976e-03],\n",
      "         [-4.4630e-02, -4.7182e-03, -3.8137e-03,  1.4168e-02,  6.6117e-03,\n",
      "           2.8264e-02, -2.8680e-02, -1.1854e-02],\n",
      "         [ 1.9541e-02,  4.8450e-03, -9.3276e-03,  1.0876e-03, -2.3662e-04,\n",
      "          -3.8718e-03,  1.1254e-02, -2.5140e-03],\n",
      "         [-3.1610e-02, -5.0117e-03,  2.2835e-03,  2.9753e-03,  1.8982e-03,\n",
      "           5.9159e-03, -1.7033e-02, -7.4563e-03],\n",
      "         [ 3.7356e-02,  2.2851e-03,  1.1755e-03, -7.8053e-03, -2.0108e-03,\n",
      "          -7.4933e-03,  2.2425e-02,  4.9184e-03],\n",
      "         [ 1.0649e-02, -8.3137e-03,  5.6060e-03,  5.7868e-04,  3.6856e-03,\n",
      "           2.2166e-03, -1.0066e-02,  8.1890e-03],\n",
      "         [-3.4465e-03,  1.1317e-02, -8.4596e-04, -2.6131e-03,  1.0731e-03,\n",
      "           3.9204e-03, -5.1204e-03,  6.1612e-03],\n",
      "         [ 2.0903e-02,  9.1253e-03, -8.3871e-04, -3.2299e-03,  1.1697e-03,\n",
      "          -1.1133e-02,  3.9301e-03,  7.2841e-03],\n",
      "         [-2.6864e-03,  1.4846e-02, -4.5323e-03,  6.1971e-03, -4.5516e-05,\n",
      "           1.2687e-02,  2.5420e-03, -4.3077e-03],\n",
      "         [-5.8073e-04, -9.6709e-03,  6.8460e-03, -2.0048e-03,  3.8038e-03,\n",
      "           2.1066e-02, -1.1649e-02, -5.7268e-03],\n",
      "         [ 2.6911e-03, -1.5718e-02, -1.6353e-02,  9.1149e-03, -1.0218e-02,\n",
      "          -2.9187e-03,  1.2686e-02, -1.9299e-02],\n",
      "         [ 1.4395e-02, -1.2410e-02,  8.9274e-03, -8.0858e-03,  3.8902e-03,\n",
      "          -3.6118e-03,  3.1731e-03,  6.9100e-03],\n",
      "         [ 1.7577e-03, -1.1682e-02,  9.4153e-03,  4.7938e-03,  3.6986e-03,\n",
      "           3.6045e-03,  1.3446e-02,  3.8790e-03],\n",
      "         [ 1.3664e-02,  7.2902e-03,  7.4069e-03, -2.1099e-03, -5.9139e-03,\n",
      "          -3.3885e-02,  9.1274e-03,  1.2248e-02],\n",
      "         [ 1.1226e-02,  1.1904e-02, -1.0015e-03,  1.0947e-03, -2.0047e-03,\n",
      "           5.1552e-03,  1.4894e-03,  8.4189e-03],\n",
      "         [-5.4393e-03, -1.0868e-02,  2.7649e-03, -7.1802e-03, -2.0773e-04,\n",
      "           3.8679e-03,  9.5319e-03, -3.0160e-03],\n",
      "         [-1.8452e-03,  4.1790e-03, -1.7006e-03, -2.8979e-03, -9.0766e-04,\n",
      "           6.9901e-03,  9.1712e-05, -7.0209e-04],\n",
      "         [ 2.5600e-02,  9.0966e-03,  1.2334e-02, -6.0872e-03,  2.8029e-03,\n",
      "          -1.6809e-02,  4.7979e-03,  1.5054e-02],\n",
      "         [ 2.0856e-02,  7.6018e-03,  1.7103e-02, -8.0853e-03,  1.8866e-03,\n",
      "          -2.2732e-02, -8.2394e-03,  1.7597e-02],\n",
      "         [-4.8010e-03, -4.5152e-03, -1.0176e-02, -1.1052e-03,  4.6027e-03,\n",
      "           1.8826e-02,  7.6825e-03, -8.0051e-03],\n",
      "         [-1.4140e-02, -6.0955e-03, -3.4100e-03,  9.7950e-05,  4.8930e-03,\n",
      "           7.0902e-03,  7.7140e-03, -6.3116e-03],\n",
      "         [ 1.4566e-02, -9.6791e-03,  1.3660e-02, -7.7810e-03,  1.4473e-02,\n",
      "           1.0780e-02, -4.2037e-03,  1.0037e-02],\n",
      "         [-2.2039e-02, -1.1261e-03,  5.3465e-03,  1.2781e-03,  3.7999e-03,\n",
      "          -1.0906e-02,  2.0994e-03,  3.3547e-03],\n",
      "         [ 2.6633e-03, -1.0769e-02,  1.6663e-02, -8.0921e-03,  9.5984e-03,\n",
      "           9.8599e-04,  1.5772e-04,  9.3922e-03],\n",
      "         [-1.3598e-02,  7.0776e-03, -2.8269e-03,  1.1829e-03, -3.1010e-03,\n",
      "           1.9333e-03,  1.4274e-02, -1.2345e-02],\n",
      "         [ 2.7782e-03,  3.2758e-03,  7.6180e-03,  6.0098e-03,  5.7053e-03,\n",
      "          -9.7871e-03, -7.2876e-03,  4.1039e-03],\n",
      "         [-2.3309e-02, -8.1160e-04, -7.0921e-03,  1.2913e-03,  2.9940e-03,\n",
      "           3.6739e-03,  3.3900e-03, -7.3384e-03],\n",
      "         [-1.4619e-02, -1.0560e-02, -1.8070e-03, -3.0777e-04,  1.8936e-03,\n",
      "           3.4291e-03,  1.5918e-03, -5.7092e-03],\n",
      "         [ 3.7195e-02,  3.8974e-03,  4.9697e-04,  1.7847e-03,  5.9669e-04,\n",
      "           5.9766e-03,  7.1742e-03,  5.6109e-03],\n",
      "         [ 8.1742e-03,  1.4863e-02, -3.8046e-03, -1.1074e-03, -2.5087e-03,\n",
      "          -3.9997e-03,  8.3179e-03, -1.4018e-03],\n",
      "         [ 1.2790e-03, -4.7250e-03,  1.2734e-02, -4.5051e-03,  9.6178e-03,\n",
      "          -1.6620e-02,  7.7035e-03,  1.3046e-02],\n",
      "         [-1.5451e-03,  5.9744e-03,  7.1082e-03, -8.0797e-03,  4.0973e-03,\n",
      "          -1.4023e-02, -9.2144e-04,  1.7109e-02],\n",
      "         [ 1.3988e-02,  1.3047e-02,  1.4651e-02, -5.4076e-03, -7.3963e-03,\n",
      "           2.4625e-03, -3.9014e-03,  6.4942e-03],\n",
      "         [-1.8790e-02,  9.8687e-04,  1.9960e-03, -2.0015e-03,  4.1048e-03,\n",
      "           6.2922e-03,  2.1846e-03, -7.2181e-04],\n",
      "         [ 4.8019e-02,  8.0220e-03,  6.8079e-03, -7.0048e-03,  2.5935e-03,\n",
      "          -3.8133e-03, -3.9164e-03,  1.3150e-02],\n",
      "         [ 4.3736e-03, -3.8140e-03, -6.9118e-03, -2.5031e-03, -3.5989e-03,\n",
      "           7.1847e-03,  2.0916e-03, -2.1550e-04],\n",
      "         [ 9.9241e-03,  4.7963e-03, -3.0411e-04,  1.8977e-03,  6.3112e-03,\n",
      "          -4.8004e-03,  1.9661e-04,  7.4056e-03]]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12128\\2567321262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[0mdrrpw_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_rob_wass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mdrrpw_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_roll_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_roll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12128\\2567321262.py\u001b[0m in \u001b[0;36mnet_roll_test\u001b[1;34m(self, X, Y, n_roll)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_perf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetParameterEstimates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mportfolio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDRRPW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mportfolio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_perf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mportfolio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12128\\3865966549.py\u001b[0m in \u001b[0;36mGetParameterEstimates\u001b[1;34m(AssetReturns, FactorReturns, technique, log)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mAssetReturns_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAssetReturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mFactorReturns_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFactorReturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAssetReturns_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import math\n",
    "\n",
    "class dist_rob_wass:\n",
    "    \"\"\"Naive 'equally-weighted' portfolio construction module\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_y, n_obs):\n",
    "        \"\"\"Naive 'equally-weighted' portfolio construction module\n",
    "\n",
    "        This object implements a basic equally-weighted investment strategy.\n",
    "\n",
    "        Inputs\n",
    "        n_x: Number of inputs (i.e., features) in the prediction model\n",
    "        n_y: Number of outputs from the prediction model\n",
    "        n_obs: Number of scenarios from which to calculate the sample set of residuals\n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.n_obs = n_obs\n",
    "\n",
    "    '''\n",
    "    Distributionally Robust Risk Parity With Wasserstein Distance Optimizer\n",
    "    Inputs: mu: numpy array, key: Symbol. value: return estimate\n",
    "            Q: nxn Asset Covariance Matrix (n: # of assets)\n",
    "    Outputs: x: optimal allocations\n",
    "\n",
    "    Formula:\n",
    "        \\min_{\\boldsymbol{\\phi} \\in \\mathcal{X}} {(\\sqrt{\\boldsymbol{\\phi}^T \\Sigma_{\\mathcal{P}}(R)\\boldsymbol{\\phi}} + \\sqrt{\\delta}||\\boldsymbol{\\phi}||_p)^2} - c\\sum_{i=1}^n ln(y)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def DRRPW(mu,Q):\n",
    "       \n",
    "        # # of Assets\n",
    "        n = len(mu)\n",
    "\n",
    "        # Decision Variables\n",
    "        w = cp.Variable(n)\n",
    "\n",
    "        # Kappa\n",
    "        k = 100\n",
    "\n",
    "        # Size of uncertainty set\n",
    "        delta = 0.05\n",
    "\n",
    "        # Norm for x\n",
    "        p = 2\n",
    "\n",
    "        constraints = [\n",
    "            w>=0 # Disallow Short Sales\n",
    "        ]\n",
    "\n",
    "        # risk = cp.quad_form(w, Q)\n",
    "\n",
    "        log_term = 0\n",
    "        for i in range(n):\n",
    "            log_term += cp.log(w[i])\n",
    "        \n",
    "        # We need to compute \\sqrt{x^T Q x} intelligently because\n",
    "        # cvxpy does not compute well with the \\sqrt\n",
    "\n",
    "        # To do this, I will take the Cholesky decomposition\n",
    "        # Q = LL^T\n",
    "        # Then, take the 2-norm of L*x\n",
    "\n",
    "        # Idea: (L_1 * x_1)^2 = Q_1 x_1\n",
    "\n",
    "        L = np.linalg.cholesky(Q)\n",
    "\n",
    "        obj = cp.power(cp.norm(L@w,2) + math.sqrt(delta)*cp.norm(w, p),2)\n",
    "        obj = obj - k*log_term\n",
    "\n",
    "        prob = cp.Problem(cp.Minimize(obj), constraints=constraints)\n",
    "        \n",
    "        # ECOS fails sometimes, if it does then do SCS\n",
    "        try:\n",
    "            prob.solve(verbose=False)\n",
    "        except:\n",
    "            prob.solve(solver='SCS',verbose=False)\n",
    "        \n",
    "        x = w.value\n",
    "        x = np.divide(x, np.sum(x))\n",
    "        \n",
    "        # Check Risk Parity Condition is actually met\n",
    "        # Note: DRRPW will not meet RP, will meet a robust version of RP\n",
    "        risk_contrib = np.multiply(x, Q.dot(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_test: Test the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(self, X, Y, n_roll=4):\n",
    "        print(Y.test())\n",
    "        \"\"\"Neural net rolling window out-of-sample test\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "        n_roll: Number of training periods (i.e., number of times to retrain the model)\n",
    "\n",
    "        Output \n",
    "        self.portfolio: add the backtest results to the e2e_net object\n",
    "        \"\"\"\n",
    "\n",
    "        # Declare backtest object to hold the test results\n",
    "        portfolio = pc.backtest(len(Y.test())-Y.n_obs, self.n_y, Y.test().index[Y.n_obs:])\n",
    "\n",
    "        test_set = DataLoader(pc.SlidingWindow(X.test(), Y.test(), self.n_obs, 0))\n",
    "\n",
    "        # Test model\n",
    "        t = 0\n",
    "        for j, (x, y, y_perf) in enumerate(test_set):\n",
    "            mu, Q = GetParameterEstimates(y, x, log=False)\n",
    "            portfolio.weights[t] = self.DRRPW(Q)\n",
    "            portfolio.rets[t] = y_perf.squeeze() @ portfolio.weights[t]\n",
    "            t += 1\n",
    "\n",
    "        # Calculate the portfolio statistics using the realized portfolio returns\n",
    "        portfolio.stats()\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "drrpw_net = dist_rob_wass(n_x, n_y, n_obs)\n",
    "drrpw_net.net_roll_test(X, Y, n_roll=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rets</th>\n",
       "      <th>tri</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-25</th>\n",
       "      <td>0.007796</td>\n",
       "      <td>1.007796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>0.010017</td>\n",
       "      <td>1.017891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-08</th>\n",
       "      <td>0.001010</td>\n",
       "      <td>1.018919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-15</th>\n",
       "      <td>0.001643</td>\n",
       "      <td>1.020593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-22</th>\n",
       "      <td>-0.007139</td>\n",
       "      <td>1.013307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-03</th>\n",
       "      <td>0.004667</td>\n",
       "      <td>3.668303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10</th>\n",
       "      <td>-0.018753</td>\n",
       "      <td>3.599510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17</th>\n",
       "      <td>-0.003671</td>\n",
       "      <td>3.586296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>0.005949</td>\n",
       "      <td>3.607630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01</th>\n",
       "      <td>-0.014932</td>\n",
       "      <td>3.553760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>454 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                rets       tri\n",
       "Date                          \n",
       "2013-01-25  0.007796  1.007796\n",
       "2013-02-01  0.010017  1.017891\n",
       "2013-02-08  0.001010  1.018919\n",
       "2013-02-15  0.001643  1.020593\n",
       "2013-02-22 -0.007139  1.013307\n",
       "...              ...       ...\n",
       "2021-09-03  0.004667  3.668303\n",
       "2021-09-10 -0.018753  3.599510\n",
       "2021-09-17 -0.003671  3.586296\n",
       "2021-09-24  0.005949  3.607630\n",
       "2021-10-01 -0.014932  3.553760\n",
       "\n",
       "[454 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ew_net.portfolio.rets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
